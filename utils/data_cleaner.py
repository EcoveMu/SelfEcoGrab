"""
è³‡æ–™æ¸…ç†èˆ‡åˆä½µå·¥å…·
åŠŸèƒ½ï¼š
1. åˆªé™¤éæœŸè³‡æ–™ï¼ˆæ ¹æ“š deadline / public_read_end / announcementEndDateï¼‰
2. åˆä½µåŒé¡å‹çˆ¬èŸ²è³‡æ–™ï¼Œå»é™¤é‡è¤‡
3. æ¯ 1000 ç­†è¼¸å‡ºä¸€å€‹æª”æ¡ˆ

Adapted for SelfEcoGrab cloud runner.
"""

import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional
import re


class DataCleaner:
    """è³‡æ–™æ¸…ç†å™¨ï¼šéæœŸè³‡æ–™åˆªé™¤ã€å»é‡ã€åˆä½µ"""
    
    # ä¸‰ç¨®çˆ¬èŸ²é¡å‹çš„è¨­å®š - é©é… SelfEcoGrab çš„æª”åæ ¼å¼
    CRAWLER_CONFIGS = {
        "tender": {
            "file_pattern": "tender_announcement_*.json",  # SelfEcoGrab format
            "date_field": "deadline",
            "id_field": "tenderId",
            "crawler_id": "tender-announcement",
            "output_prefix": "tender_merged",
        },
        "public_read": {
            "file_pattern": "public_read_*.json",
            "date_field": "public_read_end",
            "id_field": "tenderId",
            "crawler_id": "public-read",
            "output_prefix": "public_read_merged",
        },
        "promotion": {
            "file_pattern": "procurement_*.json",  # SelfEcoGrab format for ppp-mof
            "date_field": "announcementEndDate",
            "id_field": "tenderId",
            "crawler_id": "ppp-mof",
            "output_prefix": "promotion_merged",
        },
    }
    
    def __init__(self, data_dir: str = "."):
        self.data_dir = Path(data_dir)
        self.today = self._get_today_roc()
        print(f"ğŸ“… ä»Šå¤©æ—¥æœŸï¼ˆæ°‘åœ‹å¹´ï¼‰: {self.today}")
    
    def _get_today_roc(self) -> str:
        """å–å¾—ä»Šå¤©çš„æ°‘åœ‹å¹´æ—¥æœŸ (YYY/MM/DD)"""
        now = datetime.now()
        roc_year = now.year - 1911
        return f"{roc_year}/{now.month:02d}/{now.day:02d}"
    
    def _parse_roc_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ°‘åœ‹å¹´æ—¥æœŸå­—ä¸²ï¼Œè½‰æ›ç‚º datetime"""
        if not date_str or not date_str.strip():
            return None
        
        # æ¸…ç†æ—¥æœŸå­—ä¸²
        date_str = date_str.strip()
        
        # å˜—è©¦ä¸åŒæ ¼å¼
        patterns = [
            r"(\d{3})/(\d{1,2})/(\d{1,2})",  # 114/12/13
            r"(\d{3})\.(\d{1,2})\.(\d{1,2})",  # 114.12.13
            r"(\d{3})-(\d{1,2})-(\d{1,2})",  # 114-12-13
        ]
        
        for pattern in patterns:
            match = re.match(pattern, date_str)
            if match:
                try:
                    roc_year = int(match.group(1))
                    month = int(match.group(2))
                    day = int(match.group(3))
                    ad_year = roc_year + 1911
                    return datetime(ad_year, month, day)
                except ValueError:
                    continue
        
        return None
    
    def _is_expired(self, date_str: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦éæœŸ"""
        if not date_str or not date_str.strip():
            # å¦‚æœæ²’æœ‰æˆªæ­¢æ—¥æœŸï¼Œè¦–ç‚ºä¸éæœŸï¼ˆä¿ç•™è³‡æ–™ï¼‰
            return False
        
        parsed_date = self._parse_roc_date(date_str)
        if not parsed_date:
            # ç„¡æ³•è§£æçš„æ—¥æœŸï¼Œè¦–ç‚ºä¸éæœŸï¼ˆä¿ç•™è³‡æ–™ï¼‰
            return False
        
        today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        return parsed_date < today
    
    def _get_record_hash(self, record: Dict[str, Any], id_field: str) -> str:
        """è¨ˆç®—è¨˜éŒ„çš„ hashï¼Œç”¨æ–¼å»é‡"""
        # ä½¿ç”¨æ¡ˆè™Ÿ + å®Œæ•´å…§å®¹çš„ hash
        record_id = record.get(id_field, "")
        # æ’åº key ç¢ºä¿ç›¸åŒå…§å®¹ç”¢ç”Ÿç›¸åŒ hash
        content = json.dumps(record, sort_keys=True, ensure_ascii=False)
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"{record_id}_{content_hash}"
    
    def _load_json_files(self, pattern: str) -> List[Dict[str, Any]]:
        """è¼‰å…¥ç¬¦åˆ pattern çš„æ‰€æœ‰ JSON æª”æ¡ˆ"""
        files = list(self.data_dir.glob(pattern))
        all_records = []
        
        for file_path in files:
            # Skip merged files to avoid re-processing
            if "_merged_" in file_path.name:
                continue
                
            print(f"  ğŸ“‚ è®€å–: {file_path.name}")
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                # æå–è³‡æ–™
                if "data" in data:
                    records = data["data"]
                    # è™•ç† promotion é¡å‹çš„å·¢ç‹€çµæ§‹
                    if isinstance(records, dict):
                        for key, items in records.items():
                            if isinstance(items, list):
                                all_records.extend(items)
                    elif isinstance(records, list):
                        all_records.extend(records)
            except Exception as e:
                print(f"    âš  è®€å–å¤±æ•—: {e}")
        
        return all_records
    
    def _save_batches(
        self,
        records: List[Dict[str, Any]],
        prefix: str,
        crawler_id: str,
        batch_size: int = 1000
    ) -> int:
        """å°‡è¨˜éŒ„åˆ†æ‰¹å­˜æª”ï¼Œæ¯æ‰¹ batch_size ç­†"""
        if not records:
            return 0
        
        timestamp = datetime.now().strftime("%Y%m%d")
        total_batches = (len(records) + batch_size - 1) // batch_size
        
        for i in range(total_batches):
            start = i * batch_size
            end = min((i + 1) * batch_size, len(records))
            batch_records = records[start:end]
            
            filename = f"{prefix}_{timestamp}_batch{i + 1:03d}.json"
            filepath = self.data_dir / filename
            
            payload = {
                "crawlerId": crawler_id,
                "mergedAt": datetime.now().isoformat(),
                "batchNumber": i + 1,
                "totalBatches": total_batches,
                "totalRecords": len(batch_records),
                "data": batch_records,
            }
            
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            
            print(f"    ğŸ’¾ å·²å­˜æª”: {filename} ({len(batch_records)} ç­†)")
        
        return total_batches
    
    def clean_crawler_type(self, crawler_type: str) -> Dict[str, int]:
        """æ¸…ç†ç‰¹å®šé¡å‹çš„çˆ¬èŸ²è³‡æ–™"""
        if crawler_type not in self.CRAWLER_CONFIGS:
            raise ValueError(f"æœªçŸ¥çš„çˆ¬èŸ²é¡å‹: {crawler_type}")
        
        config = self.CRAWLER_CONFIGS[crawler_type]
        print(f"\n{'='*60}")
        print(f"ğŸ”§ è™•ç† {crawler_type} é¡å‹è³‡æ–™")
        print(f"{'='*60}")
        
        # 1. è¼‰å…¥æ‰€æœ‰æª”æ¡ˆ
        print(f"\nğŸ“¥ è¼‰å…¥æª”æ¡ˆ ({config['file_pattern']})...")
        records = self._load_json_files(config["file_pattern"])
        original_count = len(records)
        print(f"  âœ“ è¼‰å…¥ {original_count} ç­†è³‡æ–™")
        
        if original_count == 0:
            return {
                "original": 0,
                "after_expire": 0,
                "after_dedup": 0,
                "files": 0,
            }
        
        # 2. éæ¿¾éæœŸè³‡æ–™
        print(f"\nğŸ—‘ éæ¿¾éæœŸè³‡æ–™ ({config['date_field']})...")
        valid_records = [
            r for r in records
            if not self._is_expired(r.get(config["date_field"], ""))
        ]
        expired_count = original_count - len(valid_records)
        print(f"  âœ“ éæœŸè³‡æ–™: {expired_count} ç­†")
        print(f"  âœ“ æœ‰æ•ˆè³‡æ–™: {len(valid_records)} ç­†")
        
        # 3. å»é™¤é‡è¤‡
        print(f"\nğŸ”„ å»é™¤é‡è¤‡è³‡æ–™ ({config['id_field']})...")
        seen_hashes = set()
        unique_records = []
        
        for record in valid_records:
            record_hash = self._get_record_hash(record, config["id_field"])
            if record_hash not in seen_hashes:
                seen_hashes.add(record_hash)
                unique_records.append(record)
        
        duplicate_count = len(valid_records) - len(unique_records)
        print(f"  âœ“ é‡è¤‡è³‡æ–™: {duplicate_count} ç­†")
        print(f"  âœ“ ä¸é‡è¤‡è³‡æ–™: {len(unique_records)} ç­†")
        
        # 4. åˆ†æ‰¹å­˜æª”
        print(f"\nğŸ’¾ åˆ†æ‰¹å­˜æª” ({config['output_prefix']})...")
        files_count = self._save_batches(
            unique_records,
            config["output_prefix"],
            config["crawler_id"]
        )
        
        return {
            "original": original_count,
            "after_expire": len(valid_records),
            "after_dedup": len(unique_records),
            "files": files_count,
        }
    
    def clean_all(self) -> Dict[str, Dict[str, int]]:
        """æ¸…ç†æ‰€æœ‰é¡å‹çš„çˆ¬èŸ²è³‡æ–™"""
        results = {}
        
        for crawler_type in self.CRAWLER_CONFIGS:
            try:
                results[crawler_type] = self.clean_crawler_type(crawler_type)
            except Exception as e:
                print(f"  âš  è™•ç† {crawler_type} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                results[crawler_type] = {"error": str(e)}
        
        return results
    
    def get_merged_files(self) -> List[str]:
        """å–å¾—æ‰€æœ‰åˆä½µå¾Œçš„æª”æ¡ˆè·¯å¾‘"""
        merged_files = []
        for config in self.CRAWLER_CONFIGS.values():
            prefix = config["output_prefix"]
            pattern = f"{prefix}_*.json"
            for f in self.data_dir.glob(pattern):
                merged_files.append(str(f))
        return merged_files


def run_data_cleaner(data_dir: str = ".") -> Dict[str, Any]:
    """åŸ·è¡Œè³‡æ–™æ¸…ç†ä¸¦è¿”å›çµæœ"""
    print("\n" + "=" * 70)
    print("ğŸ“Š è³‡æ–™æ¸…ç†èˆ‡åˆä½µå·¥å…·")
    print("    åŠŸèƒ½: åˆªé™¤éæœŸè³‡æ–™ã€å»é‡è¤‡ã€åˆä½µæª”æ¡ˆ")
    print("=" * 70)
    
    cleaner = DataCleaner(data_dir)
    
    # æ¸…ç†æ‰€æœ‰é¡å‹
    results = cleaner.clean_all()
    
    # è¼¸å‡ºçµ±è¨ˆ
    print("\n" + "=" * 70)
    print("ğŸ“Š è™•ç†çµ±è¨ˆ")
    print("=" * 70)
    
    for crawler_type, stats in results.items():
        if "error" in stats:
            print(f"\nâŒ {crawler_type}: ç™¼ç”ŸéŒ¯èª¤ - {stats['error']}")
        else:
            print(f"\nâœ… {crawler_type}:")
            print(f"   åŸå§‹è³‡æ–™: {stats['original']} ç­†")
            print(f"   åˆªé™¤éæœŸ: {stats['original'] - stats['after_expire']} ç­†")
            print(f"   åˆªé™¤é‡è¤‡: {stats['after_expire'] - stats['after_dedup']} ç­†")
            print(f"   æœ€çµ‚è³‡æ–™: {stats['after_dedup']} ç­†")
            print(f"   è¼¸å‡ºæª”æ¡ˆ: {stats['files']} å€‹")
    
    print("\n" + "=" * 70)
    print("âœ… è³‡æ–™æ¸…ç†å®Œæˆï¼")
    print("=" * 70 + "\n")
    
    return {
        "stats": results,
        "merged_files": cleaner.get_merged_files()
    }


if __name__ == "__main__":
    run_data_cleaner(".")
