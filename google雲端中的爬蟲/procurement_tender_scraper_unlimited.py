"""
æ”¿åºœé›»å­æ¡è³¼ç¶² - æ‹›æ¨™å…¬å‘Šçˆ¬èŸ²
åƒè€ƒ `procurement_scraper_autopagination.py` å’Œ `public_read_scraper.py` çš„çµæ§‹ï¼Œ
é‡å° https://web.pcc.gov.tw/prkms/tender/common/basic/indexTenderBasic çš„æ‹›æ¨™æŸ¥è©¢ã€‚
"""

import json
import re
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin

from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager


class ProcurementTenderScraper:
    """æ‹›æ¨™å…¬å‘Šçˆ¬èŸ²ï¼Œè² è²¬åˆ—è¡¨æŸ¥è©¢ã€ç¿»é èˆ‡è³‡æ–™è§£æã€‚"""

    BASE_URL = "https://web.pcc.gov.tw"
    QUERY_URL = f"{BASE_URL}/prkms/tender/common/basic/indexTenderBasic"
    RESULT_URL_PATTERN = f"{BASE_URL}/prkms/tender/common/basic/readTenderBasic"

    def __init__(self, headless: bool = False, wait_seconds: int = 20):
        self.headless = headless
        self.wait_seconds = wait_seconds
        self.driver = None
        self.wait: WebDriverWait | None = None

    # ------------------------------------------------------------------ #
    # Driver lifecycle
    # ------------------------------------------------------------------ #
    def setup_driver(self):
        """åˆå§‹åŒ– Chrome WebDriverã€‚"""
        print("æ­£åœ¨åˆå§‹åŒ– Chrome WebDriver ...")
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--lang=zh-TW")
        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Safari/537.36"
        )

        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, self.wait_seconds)
        print("âœ“ Chrome WebDriver åˆå§‹åŒ–å®Œæˆ")

    def close_driver(self):
        """é—œé–‰ç€è¦½å™¨ã€‚"""
        if self.driver:
            self.driver.quit()
            self.driver = None
        print("âœ“ ç€è¦½å™¨å·²é—œé–‰")

    # ------------------------------------------------------------------ #
    # é«˜éšæµç¨‹
    # ------------------------------------------------------------------ #
    def scrape_tender_announcements(
        self,
        start_date: str | None = None,
        end_date: str | None = None,
        date_mode: str = "isNow",
        keywords: list[str] | None = None,
        max_pages: int | None = None,
        tender_type: str = "TENDER_DECLARATION",
        tender_way: str = "TENDER_WAY_ALL_DECLARATION",
        unlimited: bool = False,
        batch_size: int = 1000,
        output_prefix: str | None = None,
    ) -> list[dict]:
        """
        ä¸»æµç¨‹ï¼šåŸ·è¡ŒæŸ¥è©¢ä¸¦è‡ªå‹•ç¿»é ï¼Œå›å‚³æ‰€æœ‰æ‹›æ¨™è³‡æ–™ã€‚

        :param start_date: æ°‘åœ‹å¹´æ ¼å¼ (YYY/MM/DD)ï¼Œéœ€æ­é… date_mode='isDate'
        :param end_date: æ°‘åœ‹å¹´æ ¼å¼ (YYY/MM/DD)ï¼Œéœ€æ­é… date_mode='isDate'
        :param date_mode: isNow / isSpdt / isDate
        :param keywords: è‹¥æŒ‡å®šå‰‡ä»¥é—œéµå­—éæ¿¾ï¼ˆæ©Ÿé—œåç¨± + æ¨™æ¡ˆåç¨±ï¼‰
        :param max_pages: é™åˆ¶æœ€å¤§é æ•¸ï¼ŒNone å‰‡æŒçºŒè‡³æœ€å¾Œä¸€é 
        :param tender_type: æ‹›æ¨™é¡å‹ï¼Œé è¨­ TENDER_DECLARATION
        :param tender_way: æ‹›æ¨™æ–¹å¼ï¼Œé è¨­ TENDER_WAY_ALL_DECLARATION
        :param unlimited: True æ™‚ç„¡é æ•¸é™åˆ¶ï¼ŒæŒçºŒçˆ¬å–è‡³æœ€å¾Œä¸€é 
        :param batch_size: æ¯æ‰¹æ¬¡å­˜æª”çš„ç­†æ•¸ï¼Œé è¨­ 1000 ç­†
        :param output_prefix: è¼¸å‡ºæª”æ¡ˆå‰ç¶´ï¼Œè‹¥æŒ‡å®šå‰‡å•Ÿç”¨åˆ†æ‰¹å­˜æª”
        """
        if not self.driver or not self.wait:
            raise RuntimeError("è«‹å…ˆå‘¼å« setup_driver() åˆå§‹åŒ– WebDriver")

        # ç›´æ¥è§¸ç™¼æœå°‹ï¼ˆè¨ªå•çµæœé é¢ï¼‰ï¼Œä¸éœ€è¦å…ˆè¨ªå•æŸ¥è©¢é é¢
        self._trigger_search()

        all_items: list[dict] = []
        batch_items: list[dict] = []  # ç•¶å‰æ‰¹æ¬¡çš„è³‡æ–™
        batch_number = 1  # æ‰¹æ¬¡åºè™Ÿ
        total_saved = 0  # å·²å­˜æª”çš„ç¸½ç­†æ•¸
        
        page_index = 1
        if not unlimited:
            max_pages = max_pages or 100  # å®‰å…¨åœæï¼Œé¿å…ç„¡çª®è¿´åœˆ
        else:
            max_pages = max_pages or float('inf')  # ç„¡é™åˆ¶æ¨¡å¼

        consecutive_empty_pages = 0
        max_consecutive_empty = 3  # é€£çºŒ3é ç©ºç™½å°±åœæ­¢ï¼Œé¿å…ç„¡é™è¿´åœˆ

        while page_index <= max_pages:
            print(f"\nğŸ“„ è§£æç¬¬ {page_index} é  ...")
            page_items = self._parse_current_page(keywords=keywords)

            if not page_items:
                consecutive_empty_pages += 1
                print(f"  âš  æœ¬é æ²’æœ‰å¯è§£æçš„è³‡æ–™ï¼ˆé€£çºŒ {consecutive_empty_pages} é ç©ºç™½ï¼‰")
                if consecutive_empty_pages >= max_consecutive_empty:
                    print(f"âš  é€£çºŒ {max_consecutive_empty} é éƒ½æ²’æœ‰è³‡æ–™ï¼Œåœæ­¢çˆ¬å–ã€‚")
                    break
            else:
                consecutive_empty_pages = 0  # é‡ç½®è¨ˆæ•¸å™¨
                all_items.extend(page_items)
                batch_items.extend(page_items)
                print(f"  âœ“ æœ¬é æ“·å– {len(page_items)} ç­†ï¼Œç´¯è¨ˆ {len(all_items)} ç­†")
                
                # åˆ†æ‰¹å­˜æª”ï¼šç•¶ batch_items é”åˆ° batch_size æ™‚å­˜æª”
                if output_prefix and len(batch_items) >= batch_size:
                    self._save_batch(batch_items[:batch_size], output_prefix, batch_number)
                    total_saved += batch_size
                    batch_items = batch_items[batch_size:]  # ä¿ç•™è¶…éçš„éƒ¨åˆ†
                    batch_number += 1
                    print(f"  ğŸ“¦ å·²å­˜æª” {total_saved} ç­†ï¼ˆæ‰¹æ¬¡ {batch_number - 1}ï¼‰")

            # æª¢æŸ¥æ˜¯å¦é”åˆ°é æ•¸ä¸Šé™ï¼ˆåƒ…åœ¨éç„¡é™åˆ¶æ¨¡å¼ï¼‰
            if not unlimited and page_index >= max_pages:
                print(f"âš  é”åˆ°é è¨­å®‰å…¨ä¸Šé™ {max_pages} é ï¼Œåœæ­¢çˆ¬å–ã€‚")
                break

            # å˜—è©¦ç¿»åˆ°ä¸‹ä¸€é 
            if self._go_to_next_page(page_index):
                page_index += 1
                # _go_to_next_page å…§éƒ¨å·²ä½¿ç”¨é¡¯å¼ç­‰å¾…ï¼Œç„¡éœ€é¡å¤– sleep
            else:
                print("  âœ“ å·²åˆ°æœ€å¾Œä¸€é ")
                break

        # å­˜æª”å‰©é¤˜çš„è³‡æ–™
        if output_prefix and batch_items:
            self._save_batch(batch_items, output_prefix, batch_number)
            total_saved += len(batch_items)
            print(f"  ğŸ“¦ å·²å­˜æª”æœ€å¾Œ {len(batch_items)} ç­†ï¼ˆæ‰¹æ¬¡ {batch_number}ï¼‰")

        print(f"\nâœ… å®Œæˆï¼Œå…±æ“·å– {len(all_items)} ç­†æ‹›æ¨™å…¬å‘Šè³‡æ–™")
        if output_prefix:
            print(f"ğŸ“ å…±è¼¸å‡º {batch_number} å€‹æª”æ¡ˆï¼Œç¸½è¨ˆ {total_saved} ç­†")
        return all_items
    
    def _save_batch(self, items: list[dict], prefix: str, batch_num: int):
        """å­˜æª”å–®ä¸€æ‰¹æ¬¡çš„è³‡æ–™"""
        timestamp = datetime.now().strftime("%Y%m%d")
        filename = f"{prefix}_{timestamp}_batch{batch_num:03d}.json"
        payload = {
            "crawlerId": "tender-announcement",
            "runAt": datetime.now().isoformat(),
            "batchNumber": batch_num,
            "totalRecords": len(items),
            "data": items,
        }
        self.save_to_json(payload, filename)

    # ------------------------------------------------------------------ #
    # æŸ¥è©¢é é¢æ“ä½œ
    # ------------------------------------------------------------------ #
    def _open_query_page(self):
        assert self.driver and self.wait
        print(f"é–‹å•ŸæŸ¥è©¢é é¢ï¼š{self.QUERY_URL}")
        self.driver.get(self.QUERY_URL)
        # ç­‰å¾…æŸ¥è©¢é é¢è¼‰å…¥ï¼Œä½¿ç”¨æ ¸å¿ƒå…ƒç´ ç­‰å¾…
        try:
            self.wait.until(EC.presence_of_element_located((By.ID, "tenderTypeSelect")))
        except TimeoutException:
            # å˜—è©¦ä½¿ç”¨æ›´å¯¬é¬†çš„é¸æ“‡å™¨
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "select[id*='tenderType'], select[name*='tenderType']")))
                print("  âš  ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ°æ‹›æ¨™é¡å‹é¸æ“‡å™¨")
            except TimeoutException:
                raise RuntimeError("ç„¡æ³•è¼‰å…¥æŸ¥è©¢é é¢ï¼Œæ‰¾ä¸åˆ°æ‹›æ¨™é¡å‹é¸æ“‡å™¨ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹æ˜¯å¦æœ‰è®Šå‹•")

    def _prepare_filters(
        self,
        start_date: str | None,
        end_date: str | None,
        date_mode: str,
        tender_type: str,
        tender_way: str
    ):
        assert self.driver and self.wait

        # é¸æ“‡æ‹›æ¨™é¡å‹
        tender_type_select = Select(self.driver.find_element(By.ID, "tenderTypeSelect"))
        tender_type_select.select_by_value(tender_type)
        print(f"  âœ“ æ‹›æ¨™é¡å‹å·²è¨­å®šç‚ºï¼š{tender_type}")

        # é¸æ“‡æ‹›æ¨™æ–¹å¼
        tender_way_select = Select(self.driver.find_element(By.ID, "declarationSelect"))
        tender_way_select.select_by_value(tender_way)
        print(f"  âœ“ æ‹›æ¨™æ–¹å¼å·²è¨­å®šç‚ºï¼š{tender_way}")

        # æ—¥æœŸå€é–“
        if date_mode == "isDate" and start_date and end_date:
            try:
                date_radio = self.driver.find_element(By.ID, "level_23")  # isDate radio button
                date_radio.click()
                start_input = self.driver.find_element(By.CSS_SELECTOR, "#tenderStartDateArea input.form-date")
                end_input = self.driver.find_element(By.CSS_SELECTOR, "#tenderEndDateArea input.form-date")
                self.driver.execute_script("arguments[0].value = arguments[1];", start_input, start_date)
                self.driver.execute_script("arguments[0].value = arguments[1];", end_input, end_date)
                print(f"  âœ“ å·²è¨­å®šæ—¥æœŸå€é–“ï¼š{start_date} ~ {end_date}")
            except NoSuchElementException:
                print("  âš  æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ï¼Œæ”¹ç”¨é è¨­ã€å³æ™‚ã€æ¢ä»¶")
        elif date_mode == "isSpdt":
            try:
                self.driver.find_element(By.ID, "level_22").click()  # isSpdt radio button
                print("  âœ“ å·²åˆ‡æ›ç‚ºã€ç‰¹å®šæ—¥æœŸã€æ¨¡å¼")
            except NoSuchElementException:
                print("  âš  æ‰¾ä¸åˆ°ã€ç‰¹å®šæ—¥æœŸã€é¸é …ï¼Œæ”¹ç”¨é è¨­ã€å³æ™‚ã€æ¢ä»¶")
        else:
            try:
                self.driver.find_element(By.ID, "level_21").click()  # isNow radio button
            except NoSuchElementException:
                pass  # è‹¥æ²’æœ‰è©² radioï¼Œç¶­æŒé è¨­ç‹€æ…‹å³å¯

    def _trigger_search(self):
        assert self.driver and self.wait
        print("  â†’ åŸ·è¡ŒæŸ¥è©¢")

        # ç›´æ¥è¨ªå•çµæœé é¢ URLï¼Œé€™æ˜¯æœ€å¯é çš„æ–¹æ³•
        # é‡è¦èª¿æ•´ï¼š
        # 1. dateType=isSpdtï¼ˆç­‰æ¨™æœŸå…§ï¼‰- æŠ“å–æ‰€æœ‰æ‹›æ¨™ä¸­çš„æ¡ˆä»¶ï¼Œè€Œéåªæœ‰ç•¶æ—¥
        # 2. pageSize=100 - æ¯é é¡¯ç¤º 100 ç­†ï¼Œæ¸›å°‘ç¿»é æ¬¡æ•¸
        params = [
            "pageSize=100",  # æ¯é  100 ç­†ï¼ˆæœ€å¤§å€¼ï¼‰ï¼Œæ¸›å°‘ç¿»é æ¬¡æ•¸
            "firstSearch=true",
            "searchType=basic",
            "isBinding=N",
            "isLogIn=N",
            "level_1=on",
            "orgName=",
            "orgId=",
            "tenderName=",
            "tenderId=",
            "tenderType=TENDER_DECLARATION",
            "tenderWay=TENDER_WAY_ALL_DECLARATION",
            "dateType=isSpdt",  # ç­‰æ¨™æœŸå…§ï¼ˆæŠ“å–æ‰€æœ‰æ‹›æ¨™ä¸­çš„æ¡ˆä»¶ï¼‰
            "tenderStartDate=",
            "tenderEndDate=",
            "radProctrgCate=",
            "policyAdvocacy="
        ]

        query_string = "&".join(params)
        result_url = f"{self.RESULT_URL_PATTERN}?{query_string}"

        print(f"  â†’ è¨ªå•çµæœé é¢: {result_url}")
        self.driver.get(result_url)

        # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
        try:
            self.wait.until(EC.url_contains("readTenderBasic"))
            print("  âœ“ å·²é€²å…¥çµæœé é¢")
        except TimeoutException:
            print("  âš  URL æœªå¦‚é æœŸæ”¹è®Šï¼Œå¯èƒ½æœ‰å•é¡Œ")

        # ç­‰å¾…çµæœè¡¨æ ¼å‡ºç¾
        try:
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#tpam tbody tr")))
            print("  âœ“ æŸ¥è©¢çµæœè¡¨æ ¼è¼‰å…¥å®Œæˆ")
        except TimeoutException:
            # å¦‚æœæ‰¾ä¸åˆ° tpamï¼Œå˜—è©¦å°‹æ‰¾å…¶ä»–å¯èƒ½çš„è¡¨æ ¼çµæ§‹
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                print("  âš  ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ°è¡¨æ ¼")
            except TimeoutException:
                raise RuntimeError("æ‰¾ä¸åˆ°æŸ¥è©¢çµæœè¡¨æ ¼ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹æ˜¯å¦æœ‰è®Šå‹•")

        print("  âœ“ æŸ¥è©¢å®Œæˆï¼Œæº–å‚™é–‹å§‹æ“·å–è³‡æ–™")

    # ------------------------------------------------------------------ #
    # ç¿»é èˆ‡è§£æ
    # ------------------------------------------------------------------ #
    def _parse_current_page(self, keywords: list[str] | None = None) -> list[dict]:
        """
        å¾æœå°‹çµæœé é¢ç›´æ¥è§£ææ‹›æ¨™è³‡æ–™ã€‚
        ä¸é€²å…¥è©³ç´°é é¢ï¼Œç›´æ¥å¾åˆ—è¡¨ä¸­æå–æ‰€æœ‰å¯ç”¨è³‡è¨Šã€‚
        """
        assert self.driver
        rows = self.driver.find_elements(By.CSS_SELECTOR, "#tpam tbody tr")
        results: list[dict] = []

        for row in rows:
            cols = row.find_elements(By.TAG_NAME, "td")
            if len(cols) < 10:  # ç¢ºä¿æœ‰è¶³å¤ çš„æ¬„ä½
                continue

            # è§£æå„æ¬„ä½
            seq = cols[0].text.strip()  # é …æ¬¡
            agency = cols[1].text.strip()  # æ©Ÿé—œåç¨±

            # æ¨™æ¡ˆæ¡ˆè™Ÿå’Œåç¨±ï¼ˆåœ¨åŒä¸€æ¬„ï¼‰
            case_info = cols[2].text.strip()
            case_number = ""
            case_name = case_info
            if "\n" in case_info:
                parts = case_info.split("\n", 1)
                case_number = parts[0].strip()
                case_name = parts[1].strip()

            transmission_count = cols[3].text.strip()  # å‚³è¼¸æ¬¡æ•¸
            tender_method = cols[4].text.strip()  # æ‹›æ¨™æ–¹å¼
            procurement_type = cols[5].text.strip()  # æ¡è³¼æ€§è³ª
            announcement_date = cols[6].text.strip()  # å…¬å‘Šæ—¥æœŸ
            deadline = cols[7].text.strip()  # æˆªæ­¢æŠ•æ¨™
            budget = cols[8].text.strip()  # é ç®—é‡‘é¡

            # å–å¾—ã€Œæª¢è¦–ã€é€£çµä½œç‚º sourceUrlï¼ˆå¾åŠŸèƒ½é¸é …æ¬„ä½ï¼‰
            source_url = self._extract_detail_link(cols[9])

            # é©—è­‰å¿…è¦æ¬„ä½
            if not case_name:
                continue

            # é—œéµå­—éæ¿¾
            if keywords and not self._match_keywords(f"{agency}{case_name}", keywords):
                continue

            # ç›´æ¥å¾æœå°‹çµæœé é¢æ“·å–æ‰€æœ‰è³‡æ–™ï¼Œä¸é€²å…¥è©³ç´°é é¢
            record = {
                "serial_no": seq,
                "agency": agency,
                "tenderId": case_number,  # çµ±ä¸€ä½¿ç”¨ camelCase
                "tenderName": case_name,  # çµ±ä¸€ä½¿ç”¨ camelCase
                "transmission_count": transmission_count,
                "tender_method": tender_method,
                "procurement_type": procurement_type,
                "announcement_date": announcement_date,
                "deadline": deadline,
                "budget_amount": budget,
                "sourceUrl": source_url,  # å¾ã€Œæª¢è¦–ã€é€£çµå–å¾—
            }

            results.append(record)

        return results

    def _go_to_next_page(self, current_page: int) -> bool:
        assert self.driver and self.wait
        """
        é€šéä¿®æ”¹ URL åƒæ•¸ä¾†ç¿»é ï¼Œè€Œä¸æ˜¯é»æ“Šé€£çµã€‚
        æ”¿åºœé›»å­æ¡è³¼ç¶²ä½¿ç”¨ d-49738-p åƒæ•¸æ§åˆ¶åˆ†é ã€‚
        """
        try:
            next_page = current_page + 1
            current_url = self.driver.current_url

            # è§£æç•¶å‰ URL ä¸¦æ›´æ–°åˆ†é åƒæ•¸
            if "d-49738-p=" in current_url:
                # æ›¿æ›ç¾æœ‰çš„åˆ†é åƒæ•¸
                new_url = current_url.replace(f"d-49738-p={current_page}", f"d-49738-p={next_page}")
            else:
                # å¦‚æœæ²’æœ‰åˆ†é åƒæ•¸ï¼Œæ·»åŠ å®ƒï¼ˆé€šå¸¸ç¬¬ä¸€é æ²’æœ‰ï¼‰
                if "?" in current_url:
                    new_url = current_url + f"&d-49738-p={next_page}"
                else:
                    new_url = current_url + f"?d-49738-p={next_page}"

            print(f"  â†’ ç¿»åˆ°ç¬¬ {next_page} é : {new_url}")
            self.driver.get(new_url)

            # ç­‰å¾…é é¢è¼‰å…¥
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#tpam tbody tr")))
                print("  âœ“ æ–°é é¢è¼‰å…¥å®Œæˆ")
                return True
            except TimeoutException:
                # æª¢æŸ¥æ˜¯å¦çœŸçš„æ²’æœ‰è³‡æ–™ï¼ˆå¯èƒ½æ˜¯æœ€å¾Œä¸€é ï¼‰
                try:
                    # æª¢æŸ¥æ˜¯å¦æœ‰è¡¨æ ¼è¡Œ
                    rows = self.driver.find_elements(By.CSS_SELECTOR, "#tpam tbody tr")
                    if not rows:
                        print("  âœ“ å·²åˆ°æœ€å¾Œä¸€é ï¼ˆç„¡è³‡æ–™ï¼‰")
                        return False
                    else:
                        print("  âš  æ–°é é¢è¼‰å…¥é€¾æ™‚ï¼Œä½†æœ‰è³‡æ–™")
                        return True
                except:
                    print("  âš  ç„¡æ³•ç¢ºèªé é¢å…§å®¹")
                    return False

        except Exception as exc:
            print(f"  âœ— ç¿»é å¤±æ•—ï¼š{exc}")
            return False

    # ------------------------------------------------------------------ #
    # è¼”åŠ©å·¥å…·
    # ------------------------------------------------------------------ #
    def _extract_detail_link(self, cell) -> str | None:
        """å¾åŠŸèƒ½é¸é …æ¬„ä½æå–è©³ç´°é é¢é€£çµ"""
        try:
            # å°‹æ‰¾åŒ…å« "æª¢è¦–" çš„é€£çµ
            links = cell.find_elements(By.TAG_NAME, "a")
            for link in links:
                link_text = link.text.strip()
                if "æª¢è¦–" in link_text:
                    href = link.get_attribute("href")
                    if href and not href.lower().startswith("javascript"):
                        return urljoin(self.BASE_URL, href)
        except:
            pass
        return None

    @staticmethod
    def _match_keywords(text: str, keywords: list[str]) -> bool:
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in keywords)

    # ------------------------------------------------------------------ #
    # è¼¸å‡º
    # ------------------------------------------------------------------ #
    @staticmethod
    def save_to_json(data: list[dict], filename: str):
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON å·²å„²å­˜ï¼š{filename}")


def main():
    print("\n" + "=" * 70)
    print("ğŸ¢ æ‹›æ¨™å…¬å‘Šè‡ªå‹•åŒ–çˆ¬èŸ²")
    print("    é‡å°æ”¿åºœé›»å­æ¡è³¼ç¶²æ‹›æ¨™æŸ¥è©¢")
    print("    ğŸ“¦ æ¯ 1000 ç­†è‡ªå‹•å­˜æª”ä¸€æ¬¡")
    print("=" * 70 + "\n")

    scraper = ProcurementTenderScraper(headless=False)
    keywords = None  # ä¾‹å¦‚ï¼š["è³‡è¨Š", "é›»è…¦", "æ±¡æ°´"]
    records: list[dict] = []

    try:
        print("ğŸš€ åˆå§‹åŒ–ç€è¦½å™¨...")
        scraper.setup_driver()

        print("ğŸ” é–‹å§‹çˆ¬å–æ‹›æ¨™å…¬å‘Š...")
        # å•Ÿç”¨åˆ†æ‰¹å­˜æª”ï¼šæ¯ 1000 ç­†è¼¸å‡ºä¸€å€‹æª”æ¡ˆ
        records = scraper.scrape_tender_announcements(
            keywords=keywords, 
            unlimited=True,
            batch_size=1000,
            output_prefix="tender_batch"  # è¼¸å‡ºæª”æ¡ˆï¼štender_batch_YYYYMMDD_batch001.json
        )

        print(f"\nğŸ“ ç¸½è¨ˆæ“·å– {len(records)} ç­†è³‡æ–™")

        if records:
            unique_agencies = {item.get("agency") for item in records if item.get("agency")}
            print("\n" + "=" * 70)
            print("ğŸ“Š æ“·å–çµ±è¨ˆ")
            print("=" * 70)
            print(f"  ç¸½ç­†æ•¸ï¼š{len(records)}")
            print(f"  æ©Ÿé—œæ•¸ï¼š{len(unique_agencies)}")
            print("=" * 70 + "\n")
        else:
            print("âš ï¸  æœªæ“·å–åˆ°ä»»ä½•è³‡æ–™")

    except Exception as exc:
        print(f"\nâœ— åŸ·è¡Œå¤±æ•—ï¼š{exc}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close_driver()
        print("ğŸ›‘ ç€è¦½å™¨å·²é—œé–‰")
        if records:
            print("âœ… è¼¸å‡ºæª”æ¡ˆå·²å®Œæˆï¼Œè«‹æ–¼å°ˆæ¡ˆç›®éŒ„æª¢è¦–ã€‚")
        else:
            print("âŒ æœªç”¢ç”Ÿä»»ä½•è³‡æ–™æª”æ¡ˆã€‚")


def _build_tender_payload(
    records: List[Dict[str, Any]],
    *,
    keywords: Optional[List[str]] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    date_mode: str = "isNow",
    tender_type: str = "TENDER_DECLARATION",
    tender_way: str = "TENDER_WAY_ALL_DECLARATION",
) -> Dict[str, Any]:
    """çµ±ä¸€è¼¸å‡ºæ ¼å¼ï¼Œèˆ‡å…¶ä»–çˆ¬èŸ²ä¿æŒä¸€è‡´çš„å¤–å±¤çµæ§‹"""
    timestamp = datetime.now()
    unique_agencies = {item.get("agency") for item in records if item.get("agency")}
    return {
        "crawlerId": "tender-announcement",
        "runAt": timestamp.isoformat(),
        "filters": {
            "keywords": keywords or [],
            "dateMode": date_mode,
            "startDate": start_date,
            "endDate": end_date,
            "tenderType": tender_type,
            "tenderWay": tender_way,
        },
        "stats": {
            "totalRecords": len(records),
            "totalAgencies": len(unique_agencies),
        },
        "totalRecords": len(records),
        "data": records,
    }


def run_tender_announcement(
    *,
    headless: bool = True,
    keywords: Optional[List[str]] = None,
    max_pages: Optional[int] = None,
    date_mode: str = "isNow",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    tender_type: str = "TENDER_DECLARATION",
    tender_way: str = "TENDER_WAY_ALL_DECLARATION",
    output_dir: Optional[Path] = None,
    unlimited: bool = False
) -> Dict[str, Any]:
    """
    ä¾›å¤–éƒ¨å‘¼å«çš„å°è£å‡½å¼ï¼Œä¾¿æ–¼æ•´åˆèˆ‡è‡ªå‹•åŒ–ã€‚

    :return: dictï¼ŒåŒ…å«çµ±è¨ˆè³‡è¨Šèˆ‡è³‡æ–™åˆ—è¡¨
    """
    scraper = ProcurementTenderScraper(headless=headless)
    try:
        scraper.setup_driver()
        records = scraper.scrape_tender_announcements(
            start_date=start_date,
            end_date=end_date,
            date_mode=date_mode,
            keywords=keywords,
            max_pages=max_pages,
            tender_type=tender_type,
            tender_way=tender_way,
            unlimited=unlimited,
        )
        result = _build_tender_payload(
            records,
            keywords=keywords,
            start_date=start_date,
            end_date=end_date,
            date_mode=date_mode,
            tender_type=tender_type,
            tender_way=tender_way,
        )

        if output_dir:
            output_dir.mkdir(parents=True, exist_ok=True)
            filename = output_dir / f"tender_announcement_{datetime.now():%Y%m%d_%H%M%S}.json"
            ProcurementTenderScraper.save_to_json(result, str(filename))

        return result
    finally:
        scraper.close_driver()


if __name__ == "__main__":
    main()
