"""
æ”¿åºœé›»å­æ¡è³¼ç¶² - å…¬é–‹é–±è¦½æ¨™æ¡ˆè³‡æ–™çˆ¬èŸ²
åƒè€ƒ `procurement_scraper_autopagination.py` çš„çµæ§‹ï¼Œæ”¹ç‚ºé‡å°
https://web.pcc.gov.tw/pis/ çš„ã€Œå…¬é–‹é–±è¦½ã€æŸ¥è©¢ã€‚
"""

import json
import re
import time
import warnings
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import Select, WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

warnings.simplefilter("ignore", category=requests.packages.urllib3.exceptions.InsecureRequestWarning)  # type: ignore[attr-defined]


class PublicReadScraper:
    """å…¬é–‹é–±è¦½æ¨™æ¡ˆçˆ¬èŸ²ï¼Œè² è²¬åˆ—è¡¨æŠ“å–ã€ç¿»é èˆ‡ç´°ç¯€è§£æã€‚"""

    BASE_URL = "https://web.pcc.gov.tw"
    LIST_URL = f"{BASE_URL}/pis/"

    def __init__(self, headless: bool = False, wait_seconds: int = 20):
        self.headless = headless
        self.wait_seconds = wait_seconds
        self.driver = None
        self.wait: WebDriverWait | None = None

    # ------------------------------------------------------------------ #
    # Driver lifecycle
    # ------------------------------------------------------------------ #
    def setup_driver(self):
        """åˆå§‹åŒ– Chrome WebDriverã€‚"""
        print("æ­£åœ¨åˆå§‹åŒ– Chrome WebDriver ...")
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--lang=zh-TW")
        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Safari/537.36"
        )

        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, self.wait_seconds)
        print("âœ“ Chrome WebDriver åˆå§‹åŒ–å®Œæˆ")

    def close_driver(self):
        """é—œé–‰ç€è¦½å™¨ã€‚"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            print("âœ“ ç€è¦½å™¨å·²é—œé–‰")

    # ------------------------------------------------------------------ #
    # é«˜éšæµç¨‹
    # ------------------------------------------------------------------ #
    def scrape_public_read(
        self,
        start_date: str | None = None,
        end_date: str | None = None,
        date_mode: str = "isNow",
        keywords: list[str] | None = None,
        max_pages: int | None = None,
    ) -> list[dict]:
        """
        ä¸»æµç¨‹ï¼šåŸ·è¡ŒæŸ¥è©¢ä¸¦è‡ªå‹•ç¿»é ï¼Œå›å‚³æ‰€æœ‰æ¨™æ¡ˆè³‡æ–™ã€‚

        :param start_date: æ°‘åœ‹å¹´æ ¼å¼ (YYY/MM/DD)ï¼Œéœ€æ­é… date_mode='isDate'
        :param end_date: æ°‘åœ‹å¹´æ ¼å¼ (YYY/MM/DD)ï¼Œéœ€æ­é… date_mode='isDate'
        :param date_mode: isNow / isSpdt / isDate
        :param keywords: è‹¥æŒ‡å®šå‰‡ä»¥é—œéµå­—éæ¿¾ï¼ˆæ©Ÿé—œåç¨± + æ¨™æ¡ˆåç¨±ï¼‰
        :param max_pages: é™åˆ¶æœ€å¤§é æ•¸ï¼ŒNone å‰‡æŒçºŒè‡³æœ€å¾Œä¸€é 
        """
        if not self.driver or not self.wait:
            raise RuntimeError("è«‹å…ˆå‘¼å« setup_driver() åˆå§‹åŒ– WebDriver")

        self._open_search_page()
        self._prepare_filters(start_date, end_date, date_mode)
        self._trigger_search()

        all_items: list[dict] = []
        page_index = 1
        max_pages = max_pages or 100  # å®‰å…¨åœæï¼Œé¿å…ç„¡çª®è¿´åœˆ

        while page_index <= max_pages:
            print(f"\nğŸ“„ è§£æç¬¬ {page_index} é  ...")
            page_items = self._parse_current_page(keywords=keywords)
            if not page_items:
                print("  âš  æœ¬é æ²’æœ‰å¯è§£æçš„è³‡æ–™ï¼ŒçµæŸã€‚")
                break

            all_items.extend(page_items)
            print(f"  âœ“ æœ¬é æ“·å– {len(page_items)} ç­†ï¼Œç´¯è¨ˆ {len(all_items)} ç­†")

            if page_index >= max_pages:
                print(f"âš  é”åˆ°é è¨­å®‰å…¨ä¸Šé™ {max_pages} é ï¼Œåœæ­¢çˆ¬å–ã€‚")
                break

            if self._go_to_next_page():
                page_index += 1
                # _go_to_next_page å…§éƒ¨å·²ä½¿ç”¨é¡¯å¼ç­‰å¾…ï¼Œç„¡éœ€é¡å¤– sleep
            else:
                print("  âœ“ å·²åˆ°æœ€å¾Œä¸€é ")
                break

        print(f"\nâœ… å®Œæˆï¼Œå…±æ“·å– {len(all_items)} ç­†å…¬é–‹é–±è¦½è³‡æ–™")
        return all_items

    # ------------------------------------------------------------------ #
    # æŸ¥è©¢é é¢æ“ä½œ
    # ------------------------------------------------------------------ #
    def _open_search_page(self):
        assert self.driver and self.wait
        print(f"é–‹å•ŸæŸ¥è©¢é é¢ï¼š{self.LIST_URL}")
        self.driver.get(self.LIST_URL)
        # ç­‰å¾…æŸ¥è©¢é é¢è¼‰å…¥ï¼Œä½¿ç”¨ ID ç­‰å¾…ï¼ˆé€™æ˜¯é é¢æ ¸å¿ƒå…ƒç´ ï¼Œæ‡‰è©²ç©©å®šå­˜åœ¨ï¼‰
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå¾ŒçºŒæ“ä½œæœƒå¤±æ•—ï¼Œæ‰€ä»¥é€™è£¡çš„ç­‰å¾…æ˜¯å¿…è¦çš„
        try:
            self.wait.until(EC.presence_of_element_located((By.ID, "tenderTypeSelect")))
        except TimeoutException:
            # å˜—è©¦ä½¿ç”¨æ›´å¯¬é¬†çš„é¸æ“‡å™¨
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "select[id*='tenderType'], select[name*='tenderType']")))
                print("  âš  ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ°æ‹›æ¨™é¡å‹é¸æ“‡å™¨")
            except TimeoutException:
                raise RuntimeError("ç„¡æ³•è¼‰å…¥æŸ¥è©¢é é¢ï¼Œæ‰¾ä¸åˆ°æ‹›æ¨™é¡å‹é¸æ“‡å™¨ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹æ˜¯å¦æœ‰è®Šå‹•")

    def _prepare_filters(self, start_date: str | None, end_date: str | None, date_mode: str):
        assert self.driver and self.wait

        # é¸æ“‡ã€Œå…¬é–‹é–±è¦½ã€
        tender_type_select = Select(self.driver.find_element(By.ID, "tenderTypeSelect"))
        tender_type_select.select_by_value("PUBLIC_READ")
        print("  âœ“ æ‹›æ¨™é¡å‹å·²åˆ‡æ›ç‚ºã€Œå…¬é–‹é–±è¦½ã€")

        # æ—¥æœŸå€é–“
        if date_mode == "isDate" and start_date and end_date:
            try:
                date_radio = self.driver.find_element(By.ID, "basicIsDateDateTypeId")
                date_radio.click()
                start_input = self.driver.find_element(By.CSS_SELECTOR, "#tenderStartDateArea input.form-date")
                end_input = self.driver.find_element(By.CSS_SELECTOR, "#tenderEndDateArea input.form-date")
                self.driver.execute_script("arguments[0].value = arguments[1];", start_input, start_date)
                self.driver.execute_script("arguments[0].value = arguments[1];", end_input, end_date)
                print(f"  âœ“ å·²è¨­å®šæ—¥æœŸå€é–“ï¼š{start_date} ~ {end_date}")
            except NoSuchElementException:
                print("  âš  æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ï¼Œæ”¹ç”¨é è¨­ã€å³æ™‚ã€æ¢ä»¶")
        elif date_mode == "isSpdt":
            try:
                self.driver.find_element(By.ID, "basicIsSpdtDateTypeId").click()
                print("  âœ“ å·²åˆ‡æ›ç‚ºã€ç‰¹å®šæ—¥æœŸã€æ¨¡å¼")
            except NoSuchElementException:
                print("  âš  æ‰¾ä¸åˆ°ã€ç‰¹å®šæ—¥æœŸã€é¸é …ï¼Œæ”¹ç”¨é è¨­ã€å³æ™‚ã€æ¢ä»¶")
        else:
            try:
                self.driver.find_element(By.ID, "basicIsNowDateTypeId").click()
            except NoSuchElementException:
                pass  # è‹¥æ²’æœ‰è©² radioï¼Œç¶­æŒé è¨­ç‹€æ…‹å³å¯

    def _trigger_search(self):
        assert self.driver and self.wait
        print("  â†’ é€å‡ºæŸ¥è©¢")
        search_clicked = False

        search_locators = [
            (By.ID, "basicTenderSearchId"),
            (By.CSS_SELECTOR, "#basicTenderSearchForm a[onclick*='basicTenderSearch']"),
            (By.XPATH, "//form[@id='basicTenderSearchForm']//a[@title='æŸ¥è©¢']"),
            (By.XPATH, "(//form[@id='basicTenderSearchForm']//button[contains(text(),'æŸ¥è©¢')])[1]"),
        ]

        initial_handles = set(self.driver.window_handles)
        initial_url = self.driver.current_url

        for by, locator in search_locators:
            try:
                element = self.driver.find_element(by, locator)
                self.driver.execute_script("arguments[0].click();", element)
                search_clicked = True
                break
            except NoSuchElementException:
                continue

        if not search_clicked:
            raise RuntimeError("æ‰¾ä¸åˆ°æŸ¥è©¢æŒ‰éˆ•ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹æ˜¯å¦æœ‰è®Šå‹•")

        try:
            self.wait.until(lambda d: len(d.window_handles) > len(initial_handles))
            new_handle = next(iter(set(self.driver.window_handles) - initial_handles))
            self.driver.switch_to.window(new_handle)
        except TimeoutException:
            pass

        try:
            self.wait.until(EC.url_contains("readTenderBasic"))
        except TimeoutException:
            pass

        # ä½¿ç”¨ CSS selector ç­‰å¾…çµæœè¡¨æ ¼ï¼Œæ¯”ç›´æ¥ç­‰å¾… ID æ›´ç©©å¥
        # å¦‚æœ tpRead ID ä¸å­˜åœ¨æˆ–å‘½åä¸åŒï¼ŒCSS selector ä»å¯èƒ½æ‰¾åˆ°è¡¨æ ¼
        # ç›´æ¥ç­‰å¾…è¡¨æ ¼è¡Œå‡ºç¾ï¼Œé€™æ¨£å³ä½¿ ID æœ‰è®ŠåŒ–ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ
        try:
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#tpRead tbody tr")))
        except TimeoutException:
            # å¦‚æœæ‰¾ä¸åˆ° tpReadï¼Œå˜—è©¦å°‹æ‰¾å…¶ä»–å¯èƒ½çš„è¡¨æ ¼çµæ§‹
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                print("  âš  ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ°è¡¨æ ¼")
            except TimeoutException:
                raise RuntimeError("æ‰¾ä¸åˆ°æŸ¥è©¢çµæœè¡¨æ ¼ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹æ˜¯å¦æœ‰è®Šå‹•")
        # ä½¿ç”¨é¡¯å¼ç­‰å¾…ï¼Œç„¡éœ€é¡å¤– sleep
        print("  âœ“ æŸ¥è©¢çµæœè¼‰å…¥å®Œæˆ")

    # ------------------------------------------------------------------ #
    # ç¿»é èˆ‡è§£æ
    # ------------------------------------------------------------------ #
    def _parse_current_page(self, keywords: list[str] | None = None) -> list[dict]:
        assert self.driver
        rows = self.driver.find_elements(By.CSS_SELECTOR, "#tpRead tbody tr")
        results: list[dict] = []
        total_rows = len(rows)
        print(f"  ğŸ“„ ç™¼ç¾ {total_rows} ç­†å…¬é–‹é–±è¦½æ¡ˆä»¶")

        # é€ä¸€è™•ç†æ¯ä¸€è¡Œï¼Œæ¯æ¬¡éƒ½é‡æ–°ç²å–è¡¨æ ¼ä»¥é¿å… stale element å•é¡Œ
        for row_index in range(1, total_rows + 1):
            try:
                # æ¯æ¬¡è¿­ä»£éƒ½é‡æ–°ç²å–è¡¨æ ¼å’Œè¡Œï¼Œä»¥é¿å… stale element å•é¡Œ
                table = self.driver.find_element(By.ID, "tpRead")
                tbody = table.find_element(By.TAG_NAME, "tbody")
                current_rows = tbody.find_elements(By.TAG_NAME, "tr")

                # ç¢ºä¿è¡Œç´¢å¼•æœ‰æ•ˆ
                if row_index > len(current_rows):
                    print(f"    âš ï¸ ç¬¬ {row_index} è¡Œå·²ä¸å­˜åœ¨ï¼Œè·³é")
                    continue

                row = current_rows[row_index - 1]  # -1 å› ç‚ºåˆ—è¡¨ç´¢å¼•å¾ 0 é–‹å§‹
                cols = row.find_elements(By.TAG_NAME, "td")

                if len(cols) < 7:
                    continue

                seq = cols[0].text.strip()
                agency = cols[1].text.strip()

                tender_id = cols[2].text.strip()
                tender_id_link = self._extract_link_from_cell(cols[2])

                tender_name = cols[3].text.strip()
                announcement_count = cols[4].text.strip()

                period_text = cols[5].text.strip()
                period_start, period_end = self._parse_period(period_text)

                detail_url = self._extract_link_from_cell(cols[6]) or tender_id_link

                if keywords and not self._match_keywords(f"{agency}{tender_name}", keywords):
                    continue

                basic_info = {
                    "serial_no": seq,
                    "agency": agency,
                    "tenderId": tender_id,  # çµ±ä¸€ä½¿ç”¨ camelCase
                    "tenderName": tender_name,  # çµ±ä¸€ä½¿ç”¨ camelCaseï¼ˆèˆ‡ä¿ƒåƒä¸€è‡´ï¼‰
                    "announcement_count": announcement_count,
                    "public_read_start": period_start,
                    "public_read_end": period_end,
                    "period_raw": period_text,
                    "sourceUrl": detail_url,  # çµ±ä¸€ä½¿ç”¨ sourceUrlï¼ˆèˆ‡ä¿ƒåƒä¸€è‡´ï¼‰
                }

                # è§£æè©³ç´°é é¢è³‡è¨Š
                if detail_url:
                    print(f"    ğŸ“‹ è§£æç¬¬ {row_index}/{total_rows} ç­†æ¡ˆä»¶è©³ç´°è³‡è¨Š...")
                    print(f"      ğŸ”— è©³ç´°é é¢é€£çµï¼š{detail_url}")
                    detail_info = self._fetch_detail(detail_url)
                    if detail_info and not detail_info.get('detail_error'):
                        detail_basic = detail_info.get('detail_basic', {})
                        print(f"      âœ… å–å¾—è©³ç´°è³‡è¨Šï¼š{len(detail_basic)} å€‹æ¬„ä½")
                        if detail_basic.get('é ç®—é‡‘é¡'):
                            print(f"      ğŸ’° é ç®—é‡‘é¡ï¼š{detail_basic['é ç®—é‡‘é¡']}")
                        else:
                            print(f"      âš ï¸ æœªæ‰¾åˆ°é ç®—é‡‘é¡æ¬„ä½")
                    else:
                        error_msg = detail_info.get('detail_error', 'æœªçŸ¥éŒ¯èª¤') if detail_info else 'ç„¡è©³ç´°è³‡è¨Š'
                        print(f"      âŒ å–å¾—è©³ç´°è³‡è¨Šå¤±æ•—ï¼š{error_msg}")
                else:
                    detail_info = {}
                    print(f"      âš ï¸ ç¬¬ {row_index}/{total_rows} ç­†æ¡ˆä»¶æ²’æœ‰è©³ç´°é é¢é€£çµ")

                basic_info.update(detail_info)
                results.append(basic_info)

            except Exception as e:
                print(f"    âŒ è™•ç†ç¬¬ {row_index} è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                continue

        return results

    def _go_to_next_page(self) -> bool:
        assert self.driver and self.wait
        # å˜—è©¦æ‰¾åˆ°è¡¨æ ¼ï¼Œä½¿ç”¨æ›´å¯¬é¬†çš„é¸æ“‡å™¨
        table = None
        try:
            table = self.driver.find_element(By.ID, "tpRead")
        except NoSuchElementException:
            # å¦‚æœæ‰¾ä¸åˆ° tpRead IDï¼Œå˜—è©¦ä½¿ç”¨ CSS selector
            try:
                table = self.driver.find_element(By.CSS_SELECTOR, "#tpRead")
            except NoSuchElementException:
                # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æ‰¾ä»»ä½•åŒ…å« tbody çš„è¡¨æ ¼
                try:
                    tables = self.driver.find_elements(By.CSS_SELECTOR, "table tbody")
                    if tables:
                        # ä½¿ç”¨ç¬¬ä¸€å€‹æ‰¾åˆ°çš„è¡¨æ ¼çš„çˆ¶å…ƒç´ ï¼ˆtableï¼‰
                        table = tables[0].find_element(By.XPATH, "./..")
                except:
                    return False
        
        if not table:
            return False

        try:
            next_link = self.driver.find_element(By.XPATH, "//div[@id='pagelinks']//a[contains(text(),'ä¸‹ä¸€é ')]")
            if not next_link.is_displayed():
                return False

            self.driver.execute_script("arguments[0].click();", next_link)
            self.wait.until(EC.staleness_of(table))
            # ç­‰å¾…æ–°è¡¨æ ¼å‡ºç¾ï¼Œä½¿ç”¨æ›´å¯¬é¬†çš„é¸æ“‡å™¨
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#tpRead tbody tr")))
            except TimeoutException:
                # å¦‚æœæ‰¾ä¸åˆ° tpReadï¼Œå˜—è©¦ç­‰å¾…ä»»ä½•è¡¨æ ¼è¡Œ
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
            return True
        except NoSuchElementException:
            return False
        except TimeoutException:
            print("  âš  ç¿»é é€¾æ™‚ï¼Œåœæ­¢æ“ä½œ")
            return False

    # ------------------------------------------------------------------ #
    # ç´°ç¯€é é¢è§£æ
    # ------------------------------------------------------------------ #
    def _fetch_detail(self, detail_url: str | None) -> dict:
        if not detail_url:
            return {}

        if not self.driver or not self.wait:
            print(f"      âŒ ç„¡æ³•å–å¾—è©³ç´°è³‡è¨Šï¼šWebDriver æœªåˆå§‹åŒ–")
            return {"detail_url": detail_url, "detail_error": "WebDriver not initialized"}

        # è¨˜éŒ„ç•¶å‰é é¢ URLï¼Œä»¥ä¾¿ä¹‹å¾Œè¿”å›
        current_url = self.driver.current_url

        try:
            print(f"      ğŸŒ è¨ªå•è©³ç´°é é¢ï¼š{detail_url}")

            # ä½¿ç”¨ Selenium è¨ªå•è©³ç´°é é¢
            self.driver.get(detail_url)

            # ç­‰å¾…é é¢è¼‰å…¥ - å˜—è©¦å¤šç¨®å¯èƒ½çš„ç­‰å¾…æ¢ä»¶
            try:
                # ç­‰å¾…ä¸»è¦å…§å®¹å€åŸŸå‡ºç¾
                self.wait.until(
                    lambda d: d.find_element(By.CSS_SELECTOR, "table") or
                             d.find_element(By.ID, "printRange") or
                             len(d.find_elements(By.TAG_NAME, "table")) > 0
                )
            except TimeoutException:
                print(f"      âš ï¸ é é¢è¼‰å…¥é€¾æ™‚ï¼Œä½†ç¹¼çºŒå˜—è©¦è§£æ")

            # æª¢æŸ¥æ˜¯å¦æˆåŠŸè¼‰å…¥è©³ç´°é é¢
            page_title = self.driver.title
            print(f"      ğŸ“„ é é¢æ¨™é¡Œï¼š{page_title}")

            # å¦‚æœé é¢æ¨™é¡Œé¡¯ç¤ºéŒ¯èª¤æˆ–æœªæ‰¾åˆ°ï¼Œå˜—è©¦é‡æ–°è¼‰å…¥
            if "404" in page_title or "éŒ¯èª¤" in page_title or "Error" in page_title.lower():
                print(f"      âš ï¸ é é¢è¼‰å…¥ç•°å¸¸ï¼Œå˜—è©¦é‡æ–°æ•´ç†")
                self.driver.refresh()
                self.wait.until(
                    lambda d: d.find_element(By.CSS_SELECTOR, "table") or
                             len(d.find_elements(By.TAG_NAME, "table")) > 0
                )

            # å–å¾—é é¢åŸå§‹ç¢¼
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, "html.parser")

            detail_basic = self._parse_basic_detail_table(soup)
            attachments = self._parse_attachment_table(soup, detail_url)

            print(f"      ğŸ“„ è§£æåˆ° {len(detail_basic)} å€‹åŸºæœ¬æ¬„ä½ï¼Œ{len(attachments)} å€‹é™„ä»¶")

            # å¾å„å€‹æ¬„ä½ä¸­æå–é ç®—é‡‘é¡
            budget_amount = None
            budget_source = ""
            description = detail_basic.get("é™„åŠ èªªæ˜", "")

            # 1. å„ªå…ˆæª¢æŸ¥æ˜¯å¦æœ‰å°ˆé–€çš„é ç®—é‡‘é¡æ¬„ä½
            if detail_basic.get("é ç®—é‡‘é¡", "").strip():
                budget_amount = detail_basic["é ç®—é‡‘é¡"].strip()
                budget_source = "é ç®—é‡‘é¡æ¬„ä½"
                print(f"      ğŸ’° å¾é ç®—é‡‘é¡æ¬„ä½å–å¾—ï¼š{budget_amount}")

            # 2. å¦‚æœæ²’æœ‰ï¼Œæª¢æŸ¥é™„åŠ èªªæ˜
            if not budget_amount:
                if description:
                    extracted = self._extract_budget_from_description(description)
                    if extracted:
                        budget_amount = extracted
                        budget_source = "é™„åŠ èªªæ˜"
                        print(f"      ğŸ’° å¾é™„åŠ èªªæ˜æå–ï¼š{budget_amount}")
                    else:
                        print(f"      ğŸ“ é™„åŠ èªªæ˜é•·åº¦ï¼š{len(description)} å­—ï¼Œæœªæ‰¾åˆ°é ç®—é‡‘é¡")

            # 3. æª¢æŸ¥å…¶ä»–å¯èƒ½çš„æ¬„ä½
            if not budget_amount:
                possible_fields = ["æ¡è³¼é‡‘é¡ç´šè·", "é ç®—é‡‘é¡æ˜¯å¦å…¬é–‹", "æ±ºæ¨™é‡‘é¡", "é ç®—åƒ¹é‡‘", "å¥‘ç´„é‡‘é¡"]
                for field in possible_fields:
                    if detail_basic.get(field, "").strip():
                        value = detail_basic[field].strip()
                        # æª¢æŸ¥æ˜¯å¦åŒ…å«é‡‘é¡æ¨¡å¼
                        if "å…ƒ" in value or any(char.isdigit() for char in value):
                            budget_amount = value
                            budget_source = f"{field}æ¬„ä½"
                            print(f"      ğŸ’° å¾{field}æ¬„ä½å–å¾—ï¼š{budget_amount}")
                            break

            # æ›´æ–°æˆ–æ–°å¢é ç®—é‡‘é¡æ¬„ä½
            if budget_amount:
                detail_basic["é ç®—é‡‘é¡"] = budget_amount
                detail_basic["é ç®—é‡‘é¡ä¾†æº"] = budget_source
                print(f"      âœ… é ç®—é‡‘é¡ä¾†æºï¼š{budget_source}")
            else:
                print(f"      âš ï¸ æœªæ‰¾åˆ°ä»»ä½•é ç®—é‡‘é¡è³‡è¨Š")

            return {
                "detail_url": detail_url,
                "detail_basic": detail_basic,
                "detail_description": description,
                "attachments": attachments,
            }

        except Exception as exc:
            print(f"      âŒ è§£æè©³ç´°é é¢å¤±æ•—ï¼š{exc}")
            return {"detail_url": detail_url, "detail_error": str(exc)}

        finally:
            # ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œéƒ½è¦è¿”å›åˆ—è¡¨é é¢
            try:
                print(f"      ğŸ”™ è¿”å›åˆ—è¡¨é é¢")
                # ä½¿ç”¨ back() è¿”å›ä¸Šä¸€é ï¼Œè€Œä¸æ˜¯ç›´æ¥è¨ªå• URL
                self.driver.back()

                # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
                self.wait.until(lambda d: "tpRead" in d.current_url or "readTpRead" in d.current_url)

                # ç¢ºä¿è¡¨æ ¼å­˜åœ¨
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#tpRead tbody tr")))

                print(f"      âœ… æˆåŠŸè¿”å›åˆ—è¡¨é é¢")
            except Exception as e:
                print(f"      âš ï¸ è¿”å›åˆ—è¡¨é é¢å¤±æ•—ï¼š{e}")
                # å¦‚æœè¿”å›å¤±æ•—ï¼Œå˜—è©¦é‡æ–°è¼‰å…¥åˆ—è¡¨é é¢
                try:
                    print(f"      ğŸ”„ å˜—è©¦é‡æ–°è¼‰å…¥åˆ—è¡¨é é¢")
                    # é‡æ–°åŸ·è¡ŒæŸ¥è©¢ä¾†æ¢å¾©åˆ—è¡¨é é¢
                    self._trigger_search()
                    print(f"      âœ… é‡æ–°è¼‰å…¥åˆ—è¡¨é é¢æˆåŠŸ")
                except Exception as e2:
                    print(f"      âŒ é‡æ–°è¼‰å…¥åˆ—è¡¨é é¢ä¹Ÿå¤±æ•—ï¼š{e2}")

    @staticmethod
    def _parse_basic_detail_table(soup: BeautifulSoup) -> dict:
        detail_info: dict[str, str] = {}

        # é¦–å…ˆå˜—è©¦æ‰¾åˆ° printRange å€åŸŸï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        print_range = soup.find("div", id="printRange")
        if print_range:
            print("      ğŸ“‹ ç™¼ç¾ printRange å€åŸŸï¼Œä½¿ç”¨å„ªå…ˆè§£æ")
            tables = print_range.find_all("table")
        else:
            print("      ğŸ“‹ æœªç™¼ç¾ printRange å€åŸŸï¼Œä½¿ç”¨å…¨é é¢è¡¨æ ¼è§£æ")
            tables = soup.find_all("table")

        # è§£ææ‰€æœ‰è¡¨æ ¼
        for table_idx, table in enumerate(tables):
            print(f"      ğŸ“Š è§£æç¬¬ {table_idx + 1} å€‹è¡¨æ ¼")

            for row_idx, tr in enumerate(table.find_all("tr")):
                cells = tr.find_all("td")
                if len(cells) >= 2:
                    # å–å¾—æ¨™ç±¤å’Œå€¼
                    label_cell = cells[0]
                    value_cell = cells[1]

                    # è™•ç†æ¨™ç±¤
                    label = label_cell.get_text(strip=True)
                    if not label:
                        # å˜—è©¦å¾å…¶ä»–å…ƒç´ å–å¾—æ¨™ç±¤
                        label_elem = label_cell.find(["span", "strong", "b", "label"])
                        if label_elem:
                            label = label_elem.get_text(strip=True)

                    # è™•ç†å€¼
                    value = value_cell.get_text("\n", strip=True)

                    # å¦‚æœæ¨™ç±¤å­˜åœ¨ï¼Œå„²å­˜è³‡è¨Š
                    if label:
                        detail_info[label] = value
                        print(f"        âœ“ {label}: {value[:50]}{'...' if len(value) > 50 else ''}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡è¨Šï¼Œå˜—è©¦å…¶ä»–è§£ææ–¹å¼
        if not detail_info:
            print("      âš ï¸ è¡¨æ ¼è§£ææœªæ‰¾åˆ°è³‡è¨Šï¼Œå˜—è©¦å…¶ä»–è§£ææ–¹å¼")

            # å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é—œéµå­—çš„å…ƒç´ 
            keywords = ["é‡‘é¡", "é ç®—", "å¥‘ç´„", "æ¡è³¼", "æ¨™æ¡ˆ", "æ©Ÿé—œ", "è¯çµ¡"]
            for keyword in keywords:
                elements = soup.find_all(text=lambda text: text and keyword in text.strip())
                for elem in elements:
                    parent = elem.parent
                    if parent and parent.name in ["td", "div", "span"]:
                        text = parent.get_text(strip=True)
                        if text and len(text) > len(keyword):
                            detail_info[f"åŒ…å«{keyword}çš„æ¬„ä½"] = text
                            print(f"        âœ“ åŒ…å«{keyword}: {text[:50]}{'...' if len(text) > 50 else ''}")

        print(f"      ğŸ“Š ç¸½å…±è§£æåˆ° {len(detail_info)} å€‹æ¬„ä½")
        return detail_info

    def _parse_attachment_table(self, soup: BeautifulSoup, detail_url: str) -> list[dict]:
        attachments: list[dict] = []
        for table in soup.find_all("table"):
            headers = [th.get_text(strip=True) for th in table.find_all("th")]
            if not headers:
                continue
            if "æª”æ¡ˆåç¨±" in headers and "ä¸‹è¼‰" in headers:
                for tr in table.find_all("tr")[1:]:
                    cells = tr.find_all("td")
                    if len(cells) < 4:
                        continue
                    name = cells[1].get_text(strip=True)
                    size = cells[2].get_text(strip=True)
                    link = cells[3].find("a")
                    href = link["href"] if link and link.has_attr("href") else None
                    if not name:
                        continue
                    attachments.append(
                        {
                            "name": name,
                            "size": size,
                            "url": urljoin(detail_url, href) if href else None,
                        }
                    )
                break
        return attachments

    # ------------------------------------------------------------------ #
    # è¼”åŠ©å·¥å…·
    # ------------------------------------------------------------------ #
    @staticmethod
    def _extract_link_from_cell(cell) -> str | None:
        try:
            link = cell.find_element(By.TAG_NAME, "a")
            href = link.get_attribute("href")
            if href and not href.lower().startswith("javascript"):
                return urljoin(PublicReadScraper.BASE_URL, href)
        except NoSuchElementException:
            return None
        return None

    @staticmethod
    def _parse_period(period_text: str) -> tuple[str | None, str | None]:
        if not period_text:
            return None, None
        normalized = period_text.replace("ï¼", "-").replace("â”€", "-").replace("~", "-")
        normalized = re.sub(r"\s+", "", normalized)
        parts = re.split(r"[-è‡³]+", normalized)
        if len(parts) >= 2:
            return parts[0] or None, parts[1] or None
        return normalized or None, None

    @staticmethod
    def _extract_budget_from_description(description: str) -> str | None:
        """å¾é™„åŠ èªªæ˜ä¸­æå–é ç®—é‡‘é¡"""
        import re

        # åŒ¹é… [é ç®—é‡‘é¡]: XXXå…ƒ æˆ–é¡ä¼¼æ ¼å¼
        patterns = [
            r'\[é ç®—é‡‘é¡\]:\s*([^\[\]\n]+)',
            r'é ç®—é‡‘é¡[ï¼š:]\s*([^\n\r]+)',
            r'\[é ç®—é‡‘é¡\]([^(]+)',
        ]

        for pattern in patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                budget = match.group(1).strip()
                # æ¸…ç†å¸¸è¦‹çš„å¾Œç¶´å’Œæ‹¬è™Ÿå…§å®¹
                # å…ˆç§»é™¤åŒ…å«"å…ƒ"çš„æ‹¬è™Ÿå…§å®¹
                budget = re.sub(r'\([^)]*å…ƒ[^)]*\)', '', budget)
                budget = re.sub(r'ï¼ˆ[^ï¼‰]*å…ƒ[^ï¼‰]*ï¼‰', '', budget)
                # å†ç§»é™¤å…¶ä»–æ‹¬è™Ÿå…§å®¹
                budget = re.sub(r'\([^)]*\)', '', budget)
                budget = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', budget)
                # ç§»é™¤çµå°¾çš„"å…ƒ"å’Œç©ºç™½
                budget = re.sub(r'å…ƒ?\s*$', '', budget)
                return budget.strip()

        return None

    @staticmethod
    def _match_keywords(text: str, keywords: list[str]) -> bool:
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in keywords)

    # ------------------------------------------------------------------ #
    # è¼¸å‡º
    # ------------------------------------------------------------------ #
    @staticmethod
    def save_to_json(data: list[dict], filename: str):
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON å·²å„²å­˜ï¼š{filename}")



def main():
    print("\n" + "=" * 70)
    print("ğŸ¢ å…¬é–‹é–±è¦½æ¨™æ¡ˆè‡ªå‹•åŒ–çˆ¬èŸ²")
    print("    æœƒè‡ªå‹•åˆ‡æ›è‡³ã€Œå…¬é–‹é–±è¦½ã€ä¸¦æŒçºŒç¿»é ç›´åˆ°çµæŸ")
    print("=" * 70 + "\n")

    scraper = PublicReadScraper(headless=False)
    keywords = None  # ä¾‹å¦‚ï¼š["è³‡è¨Š", "é›»è…¦"]
    records: list[dict] = []
    run_result: dict[str, Any] = {}

    try:
        scraper.setup_driver()
        records = scraper.scrape_public_read(keywords=keywords, max_pages=None)
        run_result = _build_public_read_payload(records, keywords=keywords)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"public_read_{timestamp}.json"

        scraper.save_to_json(run_result, json_filename)

        print("\n" + "=" * 70)
        print("ğŸ“Š æ“·å–çµ±è¨ˆ")
        print("=" * 70)
        print(f"  ç­†æ•¸ï¼š{run_result.get('totalRecords', 0)}")
        unique_agencies = {item.get("agency") for item in records if item.get("agency")}
        print(f"  æ©Ÿé—œæ•¸ï¼š{len(unique_agencies)}")
        print("=" * 70 + "\n")
    except Exception as exc:  # pylint: disable=broad-except
        print(f"\nâœ— åŸ·è¡Œå¤±æ•—ï¼š{exc}")
    finally:
        scraper.close_driver()
        if records:
            print("è¼¸å‡ºæª”æ¡ˆå·²å®Œæˆï¼Œè«‹æ–¼å°ˆæ¡ˆç›®éŒ„æª¢è¦–ã€‚")
        else:
            print("æœªç”¢ç”Ÿä»»ä½•è³‡æ–™æª”æ¡ˆã€‚")


def _build_public_read_payload(
    records: List[Dict[str, Any]],
    *,
    keywords: Optional[List[str]] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    date_mode: str = "isNow"
) -> Dict[str, Any]:
    """çµ±ä¸€è¼¸å‡ºæ ¼å¼ï¼Œèˆ‡ä¿ƒåƒçˆ¬èŸ²ä¿æŒä¸€è‡´çš„å¤–å±¤çµæ§‹"""
    timestamp = datetime.now()
    unique_agencies = {item.get("agency") for item in records if item.get("agency")}
    return {
        "crawlerId": "public-read",
        "runAt": timestamp.isoformat(),
        "filters": {
            "keywords": keywords or [],
            "dateMode": date_mode,
            "startDate": start_date,
            "endDate": end_date,
        },
        "stats": {
            "totalRecords": len(records),
            "totalAgencies": len(unique_agencies),
        },
        "totalRecords": len(records),
        "data": records,
    }


def run_public_read(
    *,
    headless: bool = True,
    keywords: Optional[List[str]] = None,
    max_pages: Optional[int] = None,
    date_mode: str = "isNow",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    output_dir: Optional[Path] = None
) -> Dict[str, Any]:
    """
    ä¾›å¤–éƒ¨å‘¼å«çš„å°è£å‡½å¼ï¼Œä¾¿æ–¼æ•´åˆèˆ‡è‡ªå‹•åŒ–ã€‚

    :return: dictï¼ŒåŒ…å«çµ±è¨ˆè³‡è¨Šèˆ‡è³‡æ–™åˆ—è¡¨
    """
    scraper = PublicReadScraper(headless=headless)
    try:
        scraper.setup_driver()
        records = scraper.scrape_public_read(
            start_date=start_date,
            end_date=end_date,
            date_mode=date_mode,
            keywords=keywords,
            max_pages=max_pages,
        )
        result = _build_public_read_payload(
            records,
            keywords=keywords,
            start_date=start_date,
            end_date=end_date,
            date_mode=date_mode,
        )

        if output_dir:
            output_dir.mkdir(parents=True, exist_ok=True)
            filename = output_dir / f"public_read_{datetime.now():%Y%m%d_%H%M%S}.json"
            PublicReadScraper.save_to_json(result, str(filename))

        return result
    finally:
        scraper.close_driver()


if __name__ == "__main__":
    main()


