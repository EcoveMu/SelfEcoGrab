#ç›¸é—œä¾è³´
#pip install -U pip
#pip install selenium webdriver-manager

"""
æ”¿åºœæ ‡æ¡ˆèµ„è®¯æ”¶é›†ç³»ç»Ÿ - è©³ç´°ç‰ˆï¼ˆé»žé€²è©³æƒ…é æŠ“å–å®Œæ•´è³‡è¨Šï¼‰
æœƒé»žé€²æ¯ç­†æ¡ˆä»¶çš„è©³æƒ…é ï¼Œå–å¾—çœŸæ­£çš„ç¶²å€å’Œæ›´å¤šè³‡è¨Š
"""

import time
import json
import csv
import re
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException
from webdriver_manager.chrome import ChromeDriverManager

class ProcurementScraperDetailed:
    def __init__(self, headless=False):
        self.headless = headless
        self.driver = None
        self.wait = None
        # è¨˜ä½ç›®å‰æ­£åœ¨æŠ“çš„åˆ—è¡¨é ç¶²å€ï¼ˆå…¬å‘Šä¸­ / å·²ç™»è¼‰ï¼‰
        self.current_list_url = None
        
    def setup_driver(self):
        """è®¾å®š Chrome WebDriver"""
        print("æ­£åœ¨åˆå§‹åŒ– Chrome WebDriver...")
        
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)  # æ¸›å°‘ç­‰å¾…æ™‚é–“å¾ž20ç§’åˆ°10ç§’
        
        print("âœ“ Chrome WebDriver åˆå§‹åŒ–å®Œæˆ")
        
    def close_driver(self):
        if self.driver:
            self.driver.quit()
            print("âœ“ æµè§ˆå™¨å·²å…³é—­")
    
    def find_data_table(self):
        """å¯»æ‰¾ä¿ƒå‚å¹³å°çš„èµ„æ–™è¡¨æ ¼"""
        try:
            # å„ªå…ˆä½¿ç”¨æ›´å…·é«”çš„é¸æ“‡å™¨
            try:
                # å˜—è©¦ä½¿ç”¨å›ºå®šçš„ä¸Šå±¤ div ID
                container = self.driver.find_element(By.ID, "ContentPlaceHolder1_ListView1")
                table = container.find_element(By.CSS_SELECTOR, "table.table-rwd")
                # é©—è­‰è¡¨é ­æ˜¯å¦åŒ…å«é æœŸçš„æ¬„ä½
                headers = table.find_elements(By.TAG_NAME, "th")
                header_texts = [h.text.strip() for h in headers]
                if any(keyword in "".join(header_texts) for keyword in ["æ¡ˆä»¶åç¨±", "å…¬å‘Šæ©Ÿé—œ", "æ¡ˆä»¶ç·¨è™Ÿ"]):
                    return table
            except:
                pass
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šéæ­·æ‰€æœ‰è¡¨æ ¼ä¸¦é©—è­‰è¡¨é ­
            tables = self.driver.find_elements(By.TAG_NAME, 'table')
            for table in tables:
                table_class = table.get_attribute('class')
                if 'table-rwd' in str(table_class):
                    # åŠ ä¸€å±¤åˆ¤æ–·ï¼šæª¢æŸ¥è¡¨é ­æ–‡å­—
                    try:
                        headers = table.find_elements(By.TAG_NAME, "th")
                        header_texts = [h.text.strip() for h in headers]
                        if any(keyword in "".join(header_texts) for keyword in ["æ¡ˆä»¶åç¨±", "å…¬å‘Šæ©Ÿé—œ", "æ¡ˆä»¶ç·¨è™Ÿ"]):
                            return table
                    except:
                        continue
        except:
            pass
        return None

    def get_detail_url_from_row(self, row_index: int) -> str:
        """
        å¾žç›®å‰åˆ—è¡¨é æŒ‡å®šåˆ—ï¼Œä½¿ç”¨æ–°åˆ†é é–‹å•Ÿè©³æƒ…é å–å¾—å®˜æ–¹æ·±é€£çµã€‚
        ä¸»é é¢ä¿æŒåœ¨åŽŸä½ç½®ï¼Œä¸éœ€è¦é‡æ–°ç¿»é ã€‚
        """
        list_url = self.current_list_url or self.driver.current_url

        table = self.find_data_table()
        if not table:
            return list_url

        rows = table.find_elements(By.TAG_NAME, "tr")
        if row_index >= len(rows):
            return list_url

        row = rows[row_index]

        try:
            link_elem = row.find_element(By.TAG_NAME, "a")
        except NoSuchElementException:
            return list_url

        # å¾žé€£çµå…ƒç´ æå–URL
        try:
            detail_page_url = link_elem.get_attribute("href")
            if not detail_page_url:
                print(f"    âŒ é€£çµå…ƒç´ æ²’æœ‰ href å±¬æ€§")
                return list_url

            # ç¢ºä¿æ˜¯å®Œæ•´çš„URL
            if not detail_page_url.startswith('http'):
                detail_page_url = urljoin("https://ppp.mof.gov.tw/WWW/", detail_page_url)

        except Exception as e:
            print(f"    âŒ å–å¾—é€£çµURLå¤±æ•—: {str(e)}")
            return list_url

        # ä½¿ç”¨æ–°åˆ†é å–å¾—è©³æƒ…é URL
        original_window = self.driver.current_window_handle
        detail_url = list_url  # é è¨­å€¼

        try:
            # é–‹å•Ÿæ–°åˆ†é 
            self.driver.execute_script("window.open('', '_blank');")
            self.wait.until(lambda d: len(d.window_handles) > 1)
            windows = self.driver.window_handles
            new_window = windows[-1]  # æœ€æ–°çš„åˆ†é 

            # åˆ‡æ›åˆ°æ–°åˆ†é 
            self.driver.switch_to.window(new_window)

            # åœ¨æ–°åˆ†é ä¸­è¨ªå•è©³æƒ…é 
            self.driver.get(detail_page_url)

            # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
            self.wait.until(lambda d: d.find_element(By.TAG_NAME, "body"))

            # å–å¾—æœ€çµ‚URLï¼ˆå¯èƒ½æœ‰é‡æ–°å°Žå‘ï¼‰
            detail_url = self.driver.current_url

            # æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨æ­£ç¢ºçš„è©³æƒ…é é¢
            try:
                # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¡ˆä»¶åç¨±æˆ–å…¶ä»–æ¨™è­˜ï¼Œç¢ºèªé€™æ˜¯è©³æƒ…é é¢
                case_title_elem = self.driver.find_element(By.XPATH, "//td[contains(text(), 'æ¡ˆä»¶åç¨±')]/following-sibling::td")
                if case_title_elem and case_title_elem.text.strip():
                    # å¦‚æžœå·²ç¶“åœ¨è©³æƒ…é é¢ï¼Œç›´æŽ¥ä½¿ç”¨ç•¶å‰ URL ä½œç‚ºå°ˆå±¬é€£çµ
                    print(f"    âœ“ å·²é€²å…¥è©³æƒ…é é¢ï¼Œä½¿ç”¨ç•¶å‰ URL ä½œç‚ºå°ˆå±¬é€£çµ: {detail_url}")
                else:
                    # å¦‚æžœä¸åœ¨è©³æƒ…é é¢ï¼Œå˜—è©¦å°‹æ‰¾ oid æˆ–æ§‹é€  URL
                    if "oid=" not in detail_url:
                        html = self.driver.page_source
                        # å„ªå…ˆå˜—è©¦å¾ž HTML ä¸­æœå°‹ oid æ¨¡å¼
                        m = re.search(r"(inv_(?:ann|case)\.aspx\?oid=[0-9A-F]+)", html, re.I)
                        if m:
                            detail_url = urljoin("https://ppp.mof.gov.tw/WWW/", m.group(1))
                            print(f"    âœ“ å¾ž HTML æ‰¾åˆ° oid é€£çµ: {detail_url}")
                        else:
                            # å¦‚æžœæ‰¾ä¸åˆ° oidï¼Œå˜—è©¦å¾žæ¡ˆä»¶ç·¨è™Ÿæ§‹é€  URL
                            try:
                                case_number_elem = self.driver.find_element(By.XPATH, "//td[contains(text(), 'å·²ç°½ç´„æ¡ˆè™Ÿ') or contains(text(), 'æ¡ˆè™Ÿ')]/following-sibling::td")
                                case_number = case_number_elem.text.strip()
                                if case_number and case_number != "":
                                    # æ§‹é€  URLï¼ˆå˜—è©¦ä¸åŒçš„åƒæ•¸åç¨±ï¼‰
                                    detail_url = f"https://ppp.mof.gov.tw/WWW/inv_case.aspx?case_no={case_number}"
                                    print(f"    æ§‹é€ æ¡ˆä»¶å°ˆå±¬é€£çµ: {detail_url}")
                                else:
                                    print(f"    ç„¡æ³•å–å¾—å°ˆå±¬é€£çµï¼Œä½¿ç”¨ç•¶å‰é é¢: {detail_url}")
                            except:
                                print(f"    ç„¡æ³•å–å¾—å°ˆå±¬é€£çµï¼Œä½¿ç”¨ç•¶å‰é é¢: {detail_url}")
            except:
                # å¦‚æžœæª¢æŸ¥å¤±æ•—ï¼Œä½¿ç”¨ç•¶å‰ URL
                print(f"    è©³æƒ…é é¢æª¢æŸ¥å¤±æ•—ï¼Œä½¿ç”¨ç•¶å‰ URL: {detail_url}")

        except Exception as e:
            print(f"    âŒ æ–°åˆ†é è™•ç†å¤±æ•—: {str(e)}")
            detail_url = list_url

        finally:
            # é—œé–‰æ–°åˆ†é ä¸¦åˆ‡å›žä¸»åˆ†é 
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()  # é—œé–‰ç•¶å‰åˆ†é ï¼ˆæ–°åˆ†é ï¼‰
            except Exception as e:
                print(f"    âš ï¸ é—œé–‰æ–°åˆ†é å¤±æ•—: {str(e)}")

            # åˆ‡å›žä¸»åˆ†é 
            try:
                self.driver.switch_to.window(original_window)
            except Exception as e:
                print(f"    âš ï¸ åˆ‡å›žä¸»åˆ†é å¤±æ•—: {str(e)}")

        return detail_url

    def get_page_type(self) -> str:
        """åˆ¤æ–·ç•¶å‰è©³æƒ…é é¢é¡žåž‹ï¼ˆå…¬å‘Šä¸­/å·²ç™»è¼‰ï¼‰"""
        try:
            current_url = self.driver.current_url
            if 'inv_ann.aspx' in current_url:
                return 'announce'  # å…¬å‘Šä¸­
            elif 'inv_case.aspx' in current_url:
                return 'registered'  # å·²ç™»è¼‰
            else:
                # å˜—è©¦å¾žé é¢å…§å®¹åˆ¤æ–·
                page_text = self.driver.find_element(By.TAG_NAME, "body").text
                if 'å…¬å‘Š' in page_text and 'æˆªæ­¢' in page_text:
                    return 'announce'
                elif 'å·²ç™»è¼‰' in page_text or 'ç°½ç´„' in page_text:
                    return 'registered'
        except:
            pass
        return 'unknown'

    def get_direct_link_from_copy_button(self) -> str:
        """
        é»žæ“Š'è¤‡è£½é€£çµ'æŒ‰éˆ•ä¸¦å˜—è©¦ç²å–æ­£ç¢ºçš„ç›´é€£é€£çµ
        å°ˆæ³¨æ–¼å…¬å‘Šä¸­é é¢çš„ç›´é€£é€£çµç²å–
        """
        direct_link = ""
        try:
            # å°‹æ‰¾è¤‡è£½é€£çµæŒ‰éˆ•
            copy_button_selectors = [
                "//button[contains(text(), 'è¤‡è£½é€£çµ')]",
                "//button[contains(@onclick, 'copy')]",
                "/html/body/form/div[4]/div/div/div[2]/div[9]/button",  # å¾žç”¨æˆ¶æä¾›çš„ xpath
                "//div[contains(@class, 'pro-met')]//button"  # æ›´é€šç”¨çš„é¸æ“‡å™¨
            ]

            copy_button = None
            for selector in copy_button_selectors:
                try:
                    copy_button = self.driver.find_element(By.XPATH, selector)
                    if copy_button and copy_button.is_displayed():
                        print(f"    ðŸ“ æ‰¾åˆ°è¤‡è£½é€£çµæŒ‰éˆ•: {selector}")
                        break
                except:
                    continue

            if copy_button:
                # æª¢æŸ¥æŒ‰éˆ•çš„ onclick å±¬æ€§ï¼Œäº†è§£è¤‡è£½é‚è¼¯
                onclick_attr = copy_button.get_attribute("onclick") or ""
                print(f"    ðŸ“‹ æŒ‰éˆ• onclick å±¬æ€§: {onclick_attr[:100]}...")

                # è¨˜éŒ„é»žæ“Šå‰çš„ URL
                url_before_click = self.driver.current_url
                print(f"    ðŸ”— é»žæ“Šå‰ URL: {url_before_click}")

                    # é»žæ“Šå‰å…ˆè©³ç´°æª¢æŸ¥æŒ‰éˆ•å±¬æ€§
                print(f"    ðŸ“‹ æŒ‰éˆ•è©³ç´°è³‡è¨Š: tag={copy_button.tag_name}, type={copy_button.get_attribute('type')}")
                print(f"    ðŸ“‹ æŒ‰éˆ•onclick: {onclick_attr[:200]}...")

                # æª¢æŸ¥æ˜¯å¦èƒ½é€šéŽ JavaScript ç›´æŽ¥åŸ·è¡Œ onclick ç¨‹å¼ç¢¼
                if onclick_attr:
                    try:
                        print("    ðŸ”¬ å˜—è©¦åˆ†æž onclick ç¨‹å¼ç¢¼...")
                        # ç§»é™¤ onclick=" åŒ…è£
                        js_code = onclick_attr.strip()
                        if js_code.startswith('onclick="'):
                            js_code = js_code[9:]
                        if js_code.endswith('"'):
                            js_code = js_code[:-1]

                        print(f"    ðŸ“ æå–çš„ JS ç¨‹å¼ç¢¼: {js_code[:100]}...")

                        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„è¤‡è£½å‡½æ•¸
                        if 'copyToClipboard' in js_code or 'clipboard' in js_code.lower():
                            print("    ðŸŽ¯ æª¢æ¸¬åˆ°å‰ªè²¼ç°¿æ“ä½œï¼Œæº–å‚™æ·±å…¥åˆ†æž...")
                    except Exception as e:
                        print(f"    âš ï¸ åˆ†æž onclick ç¨‹å¼ç¢¼å¤±æ•—: {e}")

                # å°æ–¼å…¬å‘Šä¸­é é¢ï¼Œæ³¨å…¥æ›´å…¨é¢çš„æ””æˆªè…³æœ¬
                page_type = self.get_page_type()
                if page_type == 'announce':
                    try:
                        # æ³¨å…¥å…¨é¢çš„ JavaScript æ””æˆªè…³æœ¬
                        intercept_script = """
                        // æ””æˆª clipboard API
                        var copiedText = '';
                        var originalWriteText = navigator.clipboard.writeText;
                        navigator.clipboard.writeText = function(text) {
                            copiedText = text;
                            console.log('Clipboard API - è¤‡è£½å…§å®¹:', text);
                            window.copiedText = text;
                            return originalWriteText.call(this, text);
                        };

                        // æ””æˆª document.execCommand
                        var originalExecCommand = document.execCommand;
                        document.execCommand = function(command, showUI, value) {
                            if (command === 'copy') {
                                console.log('execCommand copy - å€¼:', value);
                                window.execCommandValue = value;
                            }
                            return originalExecCommand.call(this, command, showUI, value);
                        };

                        // ç›£è½ alert
                        var originalAlert = window.alert;
                        window.alert = function(message) {
                            console.log('Alert è¨Šæ¯:', message);
                            window.alertMessage = message;
                            return originalAlert.call(this, message);
                        };

                        // æä¾›å–å¾—è¤‡è£½å…§å®¹çš„å‡½æ•¸
                        window.getCopiedText = function() {
                            return copiedText || window.copiedText || window.execCommandValue || window.alertMessage || '';
                        };

                        // ç›£è½é é¢è®ŠåŒ–
                        window.beforeClickUrl = window.location.href;
                        """
                        self.driver.execute_script(intercept_script)
                        print("    ðŸ›¡ï¸ å·²æ³¨å…¥å…¨é¢æ””æˆªè…³æœ¬")
                    except Exception as e:
                        print(f"    âš ï¸ æ³¨å…¥æ””æˆªè…³æœ¬å¤±æ•—: {e}")

                # é»žæ“Šè¤‡è£½é€£çµæŒ‰éˆ•
                copy_button.click()
                print("    âœ“ å·²é»žæ“Šè¤‡è£½é€£çµæŒ‰éˆ•")

                # ç­‰å¾…ä¸€ä¸‹ï¼Œè®“è¤‡è£½æ“ä½œå®Œæˆ
                import time
                time.sleep(0.8)  # ç¨å¾®å»¶é•·ç­‰å¾…æ™‚é–“

                # æª¢æŸ¥é»žæ“Šå¾Œçš„å„ç¨®è®ŠåŒ–
                try:
                    # æª¢æŸ¥ URL æ˜¯å¦è®ŠåŒ–
                    url_after_click = self.driver.current_url
                    print(f"    ðŸ”— é»žæ“Šå¾Œ URL: {url_after_click}")

                    # æª¢æŸ¥æ˜¯å¦æœ‰ alert å‡ºç¾
                    try:
                        alert = self.driver.switch_to.alert
                        alert_text = alert.text
                        print(f"    ðŸš¨ æª¢æ¸¬åˆ° Alert: {alert_text}")
                        if 'http' in alert_text:
                            direct_link = alert_text
                            alert.accept()
                            print(f"    âœ“ å¾ž Alert ç²å–é€£çµ: {direct_link}")
                            return direct_link
                    except:
                        pass

                    # æª¢æŸ¥æ””æˆªåˆ°çš„å…§å®¹
                    if page_type == 'announce':
                        try:
                            copied_text = self.driver.execute_script("return window.getCopiedText();")
                            print(f"    ðŸ“‹ æ””æˆªåˆ°çš„å…§å®¹: '{copied_text}'")
                            if copied_text and copied_text.strip():
                                # è™•ç†è¤‡è£½çš„å…§å®¹ï¼Œå³ä½¿æ²’æœ‰ http é–‹é ­
                                if copied_text.startswith('http'):
                                    direct_link = copied_text
                                elif copied_text.startswith('//'):
                                    direct_link = 'https:' + copied_text
                                elif copied_text.startswith('ppp.mof.gov.tw'):
                                    direct_link = 'https://' + copied_text
                                elif 'inv_ann.aspx?oid=' in copied_text or 'inv_case.aspx?oid=' in copied_text:
                                    # å¦‚æžœæ˜¯ç›¸å°è·¯å¾‘ï¼Œæ·»åŠ å®Œæ•´ URL
                                    if not copied_text.startswith('http'):
                                        direct_link = urljoin("https://ppp.mof.gov.tw/WWW/", copied_text)
                                    else:
                                        direct_link = copied_text
                                else:
                                    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦æ·»åŠ  https å”è­°
                                    direct_link = 'https://' + copied_text if not copied_text.startswith('http') else copied_text

                                print(f"    âœ“ å¾žæ””æˆªå…§å®¹ç²å–é€£çµ: {direct_link}")
                                return direct_link
                        except Exception as e:
                            print(f"    âš ï¸ æª¢æŸ¥æ””æˆªå…§å®¹å¤±æ•—: {e}")

                        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…ƒç´ å‡ºç¾ï¼ˆæ¯”å¦‚è‡¨æ™‚é¡¯ç¤ºé€£çµçš„å…ƒç´ ï¼‰
                        try:
                            # æœå°‹æ‰€æœ‰å¯èƒ½é¡¯ç¤ºé€£çµçš„å…ƒç´ 
                            link_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), 'http') or contains(@value, 'http')]")
                            for elem in link_elements:
                                text_content = elem.text
                                value_content = elem.get_attribute("value") or ""
                                if ('inv_ann.aspx?oid=' in text_content or 'inv_ann.aspx?oid=' in value_content):
                                    direct_link = text_content if 'http' in text_content else value_content
                                    if not direct_link.startswith('http'):
                                        direct_link = urljoin("https://ppp.mof.gov.tw/WWW/", direct_link)
                                    print(f"    âœ“ å¾žé é¢å…ƒç´ æ‰¾åˆ°é€£çµ: {direct_link}")
                                    return direct_link
                        except Exception as e:
                            print(f"    âš ï¸ æª¢æŸ¥é é¢å…ƒç´ å¤±æ•—: {e}")

                except Exception as e:
                    print(f"    âš ï¸ æª¢æŸ¥é»žæ“Šå¾Œè®ŠåŒ–å¤±æ•—: {e}")

                # ç­‰å¾…ä¸€ä¸‹è®“é é¢æ›´æ–°
                import time
                time.sleep(0.8)  # ç¨å¾®å¢žåŠ ç­‰å¾…æ™‚é–“

                # æª¢æŸ¥æ˜¯å¦æœ‰ alert å½ˆå‡º
                try:
                    alert = self.driver.switch_to.alert
                    alert_text = alert.text
                    if 'http' in alert_text:
                        direct_link = alert_text
                        alert.accept()
                        print(f"    âœ“ å¾ž alert ç²å–é€£çµ: {direct_link}")
                        return direct_link
                except:
                    pass

                # æª¢æŸ¥ URL æ˜¯å¦æœ‰è®ŠåŒ–
                url_after_click = self.driver.current_url
                print(f"    ðŸ”— é»žæ“Šå¾Œ URL: {url_after_click}")
                if url_after_click != url_before_click and 'oid=' in url_after_click:
                    direct_link = url_after_click
                    print(f"    âœ“ URL è®ŠåŒ–ï¼Œç²å–æ–°é€£çµ: {direct_link}")
                    return direct_link

                # æª¢æŸ¥æ˜¯å¦æœ‰è‡¨æ™‚é¡¯ç¤ºçš„ input æˆ– textarea åŒ…å«é€£çµ
                try:
                    input_elements = self.driver.find_elements(By.XPATH, "//input[@type='text'][@value!=''] | //textarea[@value!='']")
                    for elem in input_elements:
                        if elem.is_displayed():
                            value = elem.get_attribute("value") or ""
                            print(f"    ðŸ“ æª¢æŸ¥ input å€¼: {value[:100]}...")
                            if 'http' in value and ('inv_ann.aspx?oid=' in value or 'inv_case.aspx?oid=' in value):
                                direct_link = value
                                print(f"    âœ“ å¾ž input å…ƒç´ ç²å–é€£çµ: {direct_link}")
                                return direct_link
                except Exception as e:
                    print(f"    âš ï¸ æª¢æŸ¥ input å…ƒç´ æ™‚å‡ºéŒ¯: {e}")

                # æª¢æŸ¥é é¢ä¸Šæ˜¯å¦æœ‰æ–°å‡ºç¾çš„é€£çµå…ƒç´ 
                try:
                    link_elements = self.driver.find_elements(By.XPATH, "//a[contains(@href, 'inv_ann.aspx?oid=') or contains(@href, 'inv_case.aspx?oid=')]")
                    for elem in link_elements:
                        href = elem.get_attribute("href")
                        if href and href != url_before_click:
                            direct_link = href
                            print(f"    âœ“ æ‰¾åˆ°æ–°çš„ç›´é€£é€£çµ: {direct_link}")
                            return direct_link
                except Exception as e:
                    print(f"    âš ï¸ æª¢æŸ¥é€£çµå…ƒç´ æ™‚å‡ºéŒ¯: {e}")

                # æª¢æŸ¥é é¢æºç¢¼ä¸­æ˜¯å¦æœ‰ç›´é€£é€£çµ
                try:
                    page_source = self.driver.page_source
                    import re

                    # å°‹æ‰¾å¯èƒ½çš„ç›´é€£é€£çµæ¨¡å¼
                    oid_patterns = [
                        r'inv_ann\.aspx\?oid=[A-F0-9]+',
                        r'inv_case\.aspx\?oid=[A-F0-9]+',
                        r'https?://ppp\.mof\.gov\.tw/WWW/(?:inv_ann|inv_case)\.aspx\?oid=[A-F0-9]+'
                    ]

                    for pattern in oid_patterns:
                        matches = re.findall(pattern, page_source, re.IGNORECASE)
                        for match in matches:
                            if not match.startswith('http'):
                                match = urljoin("https://ppp.mof.gov.tw/WWW/", match)
                            if match != url_before_click:
                                direct_link = match
                                print(f"    âœ“ å¾žé é¢æºç¢¼æ‰¾åˆ°ç›´é€£é€£çµ: {direct_link}")
                                return direct_link
                except Exception as e:
                    print(f"    âš ï¸ æª¢æŸ¥é é¢æºç¢¼æ™‚å‡ºéŒ¯: {e}")

                # å°æ–¼å…¬å‘Šä¸­é é¢ï¼Œå˜—è©¦å¾ž JavaScript ç²å–é€£çµ
                page_type = self.get_page_type()
                if page_type == 'announce':
                    try:
                        # å˜—è©¦åŸ·è¡ŒæŒ‰éˆ•çš„ onclick ç¨‹å¼ç¢¼ï¼Œä¸¦å¾žä¸­æå–é€£çµ
                        if onclick_attr:
                            print(f"    ðŸ” åˆ†æž onclick ç¨‹å¼ç¢¼ä»¥ç²å–é€£çµ...")

                            # å¸¸è¦‹çš„è¤‡è£½é€£çµ JavaScript æ¨¡å¼
                            import re

                            # å°‹æ‰¾ URL ç”Ÿæˆæ¨¡å¼
                            url_patterns = [
                                r"location\.href\s*=\s*['\"]([^'\"]+)['\"]",
                                r"window\.location\s*=\s*['\"]([^'\"]+)['\"]",
                                r"['\"](https?://[^'\"]+)['\"]",
                                r"copyToClipboard\(['\"]([^'\"]+)['\"]\)",
                                r"navigator\.clipboard\.writeText\(['\"]([^'\"]+)['\"]\)"
                            ]

                            for pattern in url_patterns:
                                matches = re.findall(pattern, onclick_attr, re.IGNORECASE)
                                for match in matches:
                                    if 'inv_ann.aspx?oid=' in match or 'inv_case.aspx?oid=' in match:
                                        if not match.startswith('http'):
                                            match = urljoin("https://ppp.mof.gov.tw/WWW/", match)
                                        direct_link = match
                                        print(f"    âœ“ å¾ž onclick ç¨‹å¼ç¢¼æå–é€£çµ: {direct_link}")
                                        return direct_link

                    except Exception as e:
                        print(f"    âš ï¸ åˆ†æž onclick ç¨‹å¼ç¢¼å¤±æ•—: {e}")

                    # å¦‚æžœç•¶å‰ URL åŒ…å« oid åƒæ•¸ï¼Œå‰‡ç›´æŽ¥ä½¿ç”¨
                    current_url = self.driver.current_url
                    if 'oid=' in current_url and 'inv_ann.aspx' in current_url:
                        direct_link = current_url
                        print(f"    âœ“ å…¬å‘Šä¸­é é¢çš„ç•¶å‰ URL ä½œç‚ºç›´é€£é€£çµ: {direct_link}")
                        return direct_link

                print("    âš ï¸ ç„¡æ³•å¾žè¤‡è£½æŒ‰éˆ•ç²å–ç›´é€£é€£çµ")

        except Exception as e:
            print(f"    âŒ ç²å–ç›´é€£é€£çµå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

        return direct_link

    def click_back_button(self) -> bool:
        """
        é»žæ“Šè¿”å›žæŒ‰éˆ•å›žåˆ°åˆ—è¡¨é 
        é€šç”¨æ–¹æ³•ï¼šå‹•æ…‹æœå°‹æ‰€æœ‰å¯èƒ½çš„è¿”å›žæŒ‰éˆ•ï¼Œä¸ä¾è³´ç‰¹å®šID
        """
        try:
            page_type = self.get_page_type()
            print(f"    ðŸ” ç•¶å‰é é¢é¡žåž‹: {page_type}")

            back_button = None
            selected_info = ""

            # ç¬¬ä¸€æ­¥ï¼šæœå°‹æ‰€æœ‰å¯èƒ½çš„ btnBack ç³»åˆ—æŒ‰éˆ•ï¼ˆå‹•æ…‹IDï¼‰
            print("    ðŸ” æœå°‹æ‰€æœ‰ btnBack ç³»åˆ—æŒ‰éˆ•...")
            try:
                # æœå°‹æ‰€æœ‰ id åŒ…å« btnBack çš„è¼¸å…¥å…ƒç´ 
                all_btnback_inputs = self.driver.find_elements(By.XPATH, "//input[contains(@id, 'btnBack')]")
                for btn in all_btnback_inputs:
                    btn_id = btn.get_attribute("id") or ""
                    if btn.is_displayed() and btn.is_enabled():
                        back_button = btn
                        selected_info = f"btnBackç³»åˆ— - ID:{btn_id}"
                        print(f"    ðŸ“ æ‰¾åˆ° btnBack æŒ‰éˆ•: {btn_id}")
                        break
            except Exception as e:
                print(f"    âš ï¸ æœå°‹ btnBack æŒ‰éˆ•å¤±æ•—: {e}")

            # ç¬¬äºŒæ­¥ï¼šå¦‚æžœæ²’æ‰¾åˆ°ï¼Œæœå°‹æ‰€æœ‰åŒ…å«è¿”å›žç›¸é—œæ–‡å­—çš„æŒ‰éˆ•
            if not back_button:
                print("    ðŸ” æœå°‹åŒ…å«è¿”å›žæ–‡å­—çš„æŒ‰éˆ•...")
                try:
                    # æœå°‹æ‰€æœ‰ input æŒ‰éˆ•
                    all_inputs = self.driver.find_elements(By.XPATH, "//input[@type='submit' or @type='button']")
                    for btn in all_inputs:
                        btn_value = btn.get_attribute("value") or ""
                        btn_id = btn.get_attribute("id") or ""
                        if ("è¿”å›ž" in btn_value or
                            "å›žä¸Šé " in btn_value or
                            "ä¸Šä¸€é " in btn_value or
                            "back" in btn_value.lower()):
                            if btn.is_displayed() and btn.is_enabled():
                                back_button = btn
                                selected_info = f"æ–‡å­—åŒ¹é…è¼¸å…¥æŒ‰éˆ• - ID:{btn_id}, å€¼:{btn_value}"
                                print(f"    ðŸ“ æ‰¾åˆ°æ–‡å­—åŒ¹é…è¿”å›žæŒ‰éˆ•: {btn_value}")
                                break
                except Exception as e:
                    print(f"    âš ï¸ æœå°‹æ–‡å­—åŒ¹é…æŒ‰éˆ•å¤±æ•—: {e}")

            # ç¬¬ä¸‰æ­¥ï¼šå¦‚æžœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œæœå°‹æ‰€æœ‰é€£çµ
            if not back_button:
                print("    ðŸ” æœå°‹åŒ…å«è¿”å›žæ–‡å­—çš„é€£çµ...")
                try:
                    all_links = self.driver.find_elements(By.XPATH, "//a")
                    for link in all_links:
                        link_text = link.text.strip()
                        link_href = link.get_attribute("href") or ""
                        if ("è¿”å›ž" in link_text or
                            "å›žä¸Šé " in link_text or
                            "ä¸Šä¸€é " in link_text or
                            "back" in link_href.lower()):
                            if link.is_displayed() and link.is_enabled():
                                back_button = link
                                selected_info = f"æ–‡å­—åŒ¹é…é€£çµ - æ–‡å­—:{link_text}"
                                print(f"    ðŸ“ æ‰¾åˆ°æ–‡å­—åŒ¹é…è¿”å›žé€£çµ: {link_text}")
                                break
                except Exception as e:
                    print(f"    âš ï¸ æœå°‹æ–‡å­—åŒ¹é…é€£çµå¤±æ•—: {e}")

            # ç¬¬å››æ­¥ï¼šå¦‚æžœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œæœå°‹æ‰€æœ‰å¯èƒ½çš„å°Žèˆªç›¸é—œå…ƒç´ 
            if not back_button:
                print("    ðŸ” æœå°‹å…¶ä»–å¯èƒ½çš„å°Žèˆªå…ƒç´ ...")
                try:
                    # æœå°‹æ‰€æœ‰å¯èƒ½çš„æŒ‰éˆ•å’Œé€£çµ
                    all_clickable = self.driver.find_elements(By.XPATH, "//input[@type='submit' or @type='button'] | //a | //button")
                    for elem in all_clickable:
                        elem_text = elem.text.strip()
                        elem_value = elem.get_attribute("value") or ""
                        elem_id = elem.get_attribute("id") or ""
                        elem_class = elem.get_attribute("class") or ""

                        # æª¢æŸ¥å„ç¨®å¯èƒ½çš„è¿”å›žæŒ‡ç¤º
                        is_back_button = (
                            "è¿”å›ž" in elem_text or "è¿”å›ž" in elem_value or
                            "å›žä¸Šé " in elem_text or "å›žä¸Šé " in elem_value or
                            "ä¸Šä¸€é " in elem_text or "ä¸Šä¸€é " in elem_value or
                            "back" in elem_id.lower() or "back" in elem_class.lower() or
                            "btnBack" in elem_id
                        )

                        if is_back_button and elem.is_displayed() and elem.is_enabled():
                            back_button = elem
                            selected_info = f"é€šç”¨åŒ¹é… - é¡žåž‹:{elem.tag_name}, ID:{elem_id}, æ–‡å­—:{elem_text or elem_value}"
                            print(f"    ðŸ“ æ‰¾åˆ°é€šç”¨åŒ¹é…è¿”å›žå…ƒç´ : {elem.tag_name} - {elem_text or elem_value}")
                            break
                except Exception as e:
                    print(f"    âš ï¸ æœå°‹é€šç”¨å°Žèˆªå…ƒç´ å¤±æ•—: {e}")

            # ç¬¬äº”æ­¥ï¼šæœ€çµ‚å‚™ç”¨æ–¹æ¡ˆ - ç€è¦½å™¨è¿”å›žåŠŸèƒ½
            if not back_button:
                print("    âŒ æ‰¾ä¸åˆ°ä»»ä½•è¿”å›žæŒ‰éˆ•ï¼Œå˜—è©¦ä½¿ç”¨ç€è¦½å™¨è¿”å›žåŠŸèƒ½...")
                try:
                    self.driver.back()
                    print("    âœ“ ä½¿ç”¨ç€è¦½å™¨è¿”å›žåŠŸèƒ½å›žåˆ°ä¸Šä¸€é ")
                    return True
                except Exception as e:
                    print(f"    âŒ ç€è¦½å™¨è¿”å›žåŠŸèƒ½ä¹Ÿå¤±æ•—: {e}")

                    # æä¾›èª¿è©¦è³‡è¨Š
                    try:
                        current_url = self.driver.current_url
                        print(f"    ðŸ”— ç•¶å‰ URL: {current_url}")

                        # åˆ†æžé é¢ä¸Šçš„æ‰€æœ‰å¯é»žæ“Šå…ƒç´ 
                        all_clickable = self.driver.find_elements(By.XPATH, "//input[@type='submit'] | //button | //a")
                        clickable_info = []
                        for elem in all_clickable[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                            elem_text = elem.text.strip()
                            elem_value = elem.get_attribute("value") or ""
                            elem_id = elem.get_attribute("id") or ""
                            if elem_text or elem_value or elem_id:
                                clickable_info.append(f"{elem.tag_name}[{elem_id}]: '{elem_text or elem_value}'")

                        if clickable_info:
                            print(f"    ðŸ“‹ é é¢ä¸Šæ‰¾åˆ°çš„å¯é»žæ“Šå…ƒç´ : {clickable_info}")
                    except Exception as debug_e:
                        print(f"    âš ï¸ ç„¡æ³•ç²å–èª¿è©¦è³‡è¨Š: {debug_e}")

                    return False

            # é»žæ“Šæ‰¾åˆ°çš„è¿”å›žæŒ‰éˆ•
            try:
                back_button.click()
                print(f"    âœ“ å·²é»žæ“Šè¿”å›žæŒ‰éˆ• ({page_type}) - {selected_info}")
                return True
            except Exception as click_e:
                print(f"    âŒ é»žæ“Šè¿”å›žæŒ‰éˆ•å¤±æ•—: {click_e}")
                return False

        except Exception as e:
            print(f"    âŒ è¿”å›žæŒ‰éˆ•è™•ç†éŽç¨‹å‡ºéŒ¯: {str(e)}")
            return False

    def extract_detail_info(self) -> Dict[str, str]:
        """
        å¾žè©³æƒ…é å¿«é€ŸæŠ“å–é¡å¤–è³‡è¨Šï¼ˆé ç®—ã€æ¡ˆè™Ÿç­‰ï¼‰
        å„ªåŒ–ç‰ˆæœ¬ï¼šæ¸›å°‘ä¸å¿…è¦çš„ç­‰å¾…å’Œè¼¸å‡º
        """
        detail_info = {
            'detailCaseNumber': '',
            'budget': '',
            'budgetAmount': None
        }

        try:
            # å¿«é€Ÿæœå°‹é ç®—è³‡è¨Šï¼ˆå„ªå…ˆé †åºï¼‰
            budget_selectors = [
                "//td[contains(text(), 'æ°‘é–“æŠ•è³‡é‡‘é¡')]/following-sibling::td",
                "//td[contains(text(), 'é ç®—')]/following-sibling::td",
                "//td[contains(text(), 'é‡‘é¡')]/following-sibling::td"
            ]

            for selector in budget_selectors:
                try:
                    budget_elem = self.driver.find_element(By.XPATH, selector)
                    budget_text = budget_elem.text.strip()
                    if budget_text and budget_text != "":
                        detail_info['budget'] = budget_text
                        break
                except:
                    continue

            # å¦‚æžœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦å¾žé é¢æ–‡å­—ä¸­æœå°‹
            if not detail_info['budget']:
                try:
                    page_text = self.driver.find_element(By.TAG_NAME, "body").text
                    amount_pattern = r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*å…ƒ'
                    match = re.search(amount_pattern, page_text)
                    if match:
                        detail_info['budget'] = match.group(1) + "å…ƒ"
                except:
                    pass

            # è§£æžé‡‘é¡æ•¸å­—
            if detail_info['budget']:
                try:
                    budget_number = re.sub(r'[^\d.]', '', detail_info['budget'])
                    if budget_number:
                        detail_info['budgetAmount'] = float(budget_number)
                except:
                    pass

            # æœå°‹æ¡ˆè™Ÿ
            case_number_selectors = [
                "//td[contains(text(), 'å·²ç°½ç´„æ¡ˆè™Ÿ')]/following-sibling::td",
                "//td[contains(text(), 'æ¨™æ¡ˆæ¡ˆè™Ÿ')]/following-sibling::td",
                "//td[contains(text(), 'æ¡ˆè™Ÿ')]/following-sibling::td"
            ]

            for selector in case_number_selectors:
                try:
                    case_elem = self.driver.find_element(By.XPATH, selector)
                    case_text = case_elem.text.strip()
                    if case_text and case_text != "":
                        detail_info['detailCaseNumber'] = case_text
                        break
                except:
                    continue

        except Exception as e:
            # å¦‚æžœç™¼ç”ŸéŒ¯èª¤ï¼Œéœé»˜è™•ç†ï¼Œä¸å½±éŸ¿ä¸»è¦æµç¨‹
            pass

        return detail_info

    def get_detail_url_and_info_from_row(self, row_index: int, current_page: int = 1) -> tuple:
        """
        å¾žç›®å‰åˆ—è¡¨é æŒ‡å®šåˆ—å–å¾—è©³æƒ…è³‡è¨Šã€‚
        ä½¿ç”¨å„ªåŒ–çš„é é¢åˆ‡æ›æ–¹å¼ï¼šé»žæ“Šé€£çµé€²å…¥è©³æƒ…é ï¼ŒæŠ“å–è³‡è¨Šå’Œç›´é€£é€£çµï¼Œç„¶å¾Œé»žæ“Šè¿”å›žæŒ‰éˆ•å›žåˆ°åŽŸé é¢ã€‚
        """
        list_url = self.current_list_url or self.driver.current_url
        detail_info = {}
        detail_url = list_url  # é è¨­å€¼

        table = self.find_data_table()
        if not table:
            return list_url, detail_info

        rows = table.find_elements(By.TAG_NAME, "tr")
        if row_index >= len(rows):
            return list_url, detail_info

        row = rows[row_index]

        try:
            link_elem = row.find_element(By.TAG_NAME, "a")
        except NoSuchElementException:
            return list_url, detail_info

        # å¾žé€£çµå…ƒç´ æå–URL
        try:
            detail_page_url = link_elem.get_attribute("href")
            if not detail_page_url:
                return list_url, detail_info

            # ç¢ºä¿æ˜¯å®Œæ•´çš„URL
            if not detail_page_url.startswith('http'):
                detail_page_url = urljoin("https://ppp.mof.gov.tw/WWW/", detail_page_url)

        except Exception as e:
            return list_url, detail_info

        # ä½¿ç”¨å„ªåŒ–çš„é é¢åˆ‡æ›æ–¹å¼ï¼šé€²å…¥è©³æƒ…é ï¼ŒæŠ“å–è³‡è¨Šï¼Œé»žæ“Šè¿”å›žæŒ‰éˆ•
        try:
            # é»žæ“Šé€£çµé€²å…¥è©³æƒ…é 
            link_elem.click()

            # ç­‰å¾…é é¢è¼‰å…¥
            self.wait.until(lambda d: d.find_element(By.TAG_NAME, "body"))

            # å–å¾—è©³æƒ…é URLå’Œè³‡è¨Š
            detail_url = self.driver.current_url
            detail_info = self.extract_detail_info()

            # æª¢æŸ¥ç•¶å‰ URL æ˜¯å¦å·²ç¶“æ˜¯ç›´é€£é€£çµæ ¼å¼
            page_type = self.get_page_type()
            if page_type == 'announce' and 'inv_ann.aspx?oid=' in detail_url:
                print(f"    âœ“ ç•¶å‰ URL å·²æ˜¯å…¬å‘Šä¸­ç›´é€£é€£çµ: {detail_url}")
                # ä¸éœ€è¦é€²ä¸€æ­¥è™•ç†ï¼Œç›´æŽ¥ä½¿ç”¨ç•¶å‰ URL
            elif page_type == 'registered' and 'inv_case.aspx?oid=' in detail_url:
                print(f"    âœ“ ç•¶å‰ URL å·²æ˜¯å·²ç™»è¼‰ç›´é€£é€£çµ: {detail_url}")
                # ä¸éœ€è¦é€²ä¸€æ­¥è™•ç†ï¼Œç›´æŽ¥ä½¿ç”¨ç•¶å‰ URL
            else:
                # å˜—è©¦é»žæ“Šè¤‡è£½é€£çµæŒ‰éˆ•ç²å–æ­£ç¢ºçš„ç›´é€£é€£çµ
                direct_link = self.get_direct_link_from_copy_button()
                if direct_link:
                    detail_url = direct_link
                    print(f"    ðŸ”„ æ›´æ–°ç‚ºè¤‡è£½æŒ‰éˆ•ç²å–çš„é€£çµ: {detail_url}")
                else:
                    print(f"    âš ï¸ ç„¡æ³•ç²å–ç›´é€£é€£çµï¼Œä½¿ç”¨ç•¶å‰ URL: {detail_url}")

            # ä½¿ç”¨è¿”å›žæŒ‰éˆ•å›žåˆ°åˆ—è¡¨é 
            self.click_back_button()

            # ç­‰å¾…å›žåˆ°åˆ—è¡¨é 
            self.wait.until(lambda d: self.find_data_table() is not None)

        except Exception as e:
            print(f"    âš ï¸ è©³æƒ…é è™•ç†å¤±æ•—: {str(e)}")
            # å¦‚æžœå‡ºéŒ¯ï¼Œå˜—è©¦ä½¿ç”¨è¿”å›žæŒ‰éˆ•æˆ–é‡æ–°è¼‰å…¥åˆ—è¡¨é 
            try:
                self.click_back_button()
                self.wait.until(lambda d: self.find_data_table() is not None)
            except:
                # å¦‚æžœè¿”å›žæŒ‰éˆ•ä¹Ÿå¤±æ•—ï¼Œé‡æ–°è¼‰å…¥åˆ—è¡¨é 
                try:
                    self.driver.get(list_url)
                    self.wait.until(lambda d: self.find_data_table() is not None)
                except:
                    pass
            detail_url = list_url

        return detail_url, detail_info

    def navigate_to_page(self, target_page: int) -> bool:
        """æ™ºæ…§ç¿»é åˆ°æŒ‡å®šé é¢"""
        try:
            # æª¢æŸ¥ç•¶å‰é é¢æ˜¯å¦å·²ç¶“æ˜¯ç›®æ¨™é é¢
            current_page = self._get_current_page_number()
            if current_page == target_page:
                print(f"    âœ… å·²ç¶“åœ¨ç¬¬ {target_page} é ")
                return True

            # å°‹æ‰¾å¯è¦‹çš„é é¢æŒ‰éˆ•
            page_buttons = self.driver.find_elements(By.CSS_SELECTOR, "a.imgPage.nuimgPage")
            available_pages = []

            for button in page_buttons:
                try:
                    page_text = button.text.strip()
                    if page_text.isdigit():
                        page_num = int(page_text)
                        available_pages.append((page_num, button))
                except:
                    continue

            print(f"    ðŸ“„ å¯è¦‹é é¢æŒ‰éˆ•: {[p for p, _ in available_pages]}")

            # æª¢æŸ¥ç›®æ¨™é é¢æ˜¯å¦åœ¨å¯è¦‹ç¯„åœå…§
            for page_num, button in available_pages:
                if page_num == target_page:
                    print(f"    ðŸ–±ï¸ ç›´æŽ¥é»žæ“Šç¬¬ {target_page} é æŒ‰éˆ•")
                    button.click()
                    self.wait.until(EC.staleness_of(self.find_data_table()))
                    self.wait.until(lambda d: self.find_data_table() is not None)
                    print(f"    âœ… å·²è·³åˆ°ç¬¬ {target_page} é ")
                    return True

            # å¦‚æžœç›®æ¨™é é¢ä¸åœ¨å¯è¦‹ç¯„åœå…§ï¼Œä½¿ç”¨é€æ­¥ç¿»é 
            print(f"    ðŸ“„ ç¬¬ {target_page} é ä¸åœ¨å¯è¦‹ç¯„åœï¼Œä½¿ç”¨é€æ­¥ç¿»é ")
            current_page = self._get_current_page_number()

            if target_page > current_page:
                # å¾€å¾Œç¿»
                pages_to_flip = target_page - current_page
                for _ in range(pages_to_flip):
                    if not self.has_next_page():
                        print(f"    âŒ ç„¡æ³•ç¹¼çºŒç¿»é ")
                        return False
                    if not self.click_next_page():
                        print(f"    âŒ ç¿»é å¤±æ•—")
                        return False
            else:
                # å¾€å‰ç¿»ï¼ˆå¦‚æžœæœ‰çš„è©±ï¼‰
                print(f"    âš ï¸ ä¸æ”¯æ´å¾€å‰ç¿»é ï¼Œåœç•™åœ¨ç•¶å‰é é¢")
                return False

            return True

        except Exception as e:
            print(f"    âŒ æ™ºæ…§ç¿»é å¤±æ•—: {str(e)}")
            return False

    def _get_current_page_number(self) -> int:
        """ç²å–ç•¶å‰é ç¢¼ï¼ˆç›¡å¯èƒ½ç²¾ç¢ºï¼‰"""
        try:
            # æª¢æŸ¥å“ªå€‹é é¢æŒ‰éˆ•æœ‰ active æˆ– current é¡žåˆ¥
            page_buttons = self.driver.find_elements(By.CSS_SELECTOR, "a.imgPage.nuimgPage")
            for button in page_buttons:
                classes = button.get_attribute("class") or ""
                if "active" in classes.lower() or "current" in classes.lower():
                    try:
                        return int(button.text.strip())
                    except:
                        continue

            # å¦‚æžœæ²’æœ‰æ‰¾åˆ° active æŒ‰éˆ•ï¼Œæª¢æŸ¥ URL æˆ–å…¶ä»–è·¡è±¡
            # æˆ–è€…å‡è¨­åœ¨ç¬¬ä¸€é ï¼ˆå› ç‚ºæˆ‘å€‘ç¸½æ˜¯å¾žç¬¬ä¸€é é‡æ–°é–‹å§‹ï¼‰
            return 1

        except:
            return 1

    def has_next_page(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆä¸”å¯ç‚¹å‡»ï¼‰"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨æ­£ç¢ºçš„ xpathï¼ˆæ ¹æ“šå¯¦éš›ç¶²é çµæ§‹ï¼‰
            next_button = self.driver.find_element(By.XPATH, '//*[@id="ContentPlaceHolder1_ListView1_DataPager1"]/input[2]')

            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç”¨ï¼ˆä¸æ˜¯ disabledï¼‰
            is_enabled = next_button.is_enabled()
            is_displayed = next_button.is_displayed()

            # æ£€æŸ¥æŒ‰é’®çš„ class æ˜¯å¦åŒ…å« disabled ç›¸å…³
            button_class = next_button.get_attribute('class') or ''
            is_disabled_class = 'aspNetDisabled' in button_class or 'disable' in button_class.lower()

            print(f"  æª¢æŸ¥ä¸‹ä¸€é æŒ‰éˆ•: å•Ÿç”¨={is_enabled}, é¡¯ç¤º={is_displayed}, é¡žåˆ¥='{button_class}', åœç”¨é¡žåˆ¥={is_disabled_class}")
            return is_enabled and is_displayed and not is_disabled_class

        except Exception as e:
            print(f"  æ‰¾ä¸åˆ°ä¸‹ä¸€é æŒ‰éˆ•: {str(e)}")
            return False
    
    def click_next_page(self):
        """ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œä½¿ç”¨æ˜¾å¼ç­‰å¾…æ›¿ä»£ time.sleep"""
        table = self.find_data_table()
        if not table:
            return False

        try:
            # æ–¹æ³• 1: ä½¿ç”¨æ­£ç¢ºçš„ XPathï¼ˆå„ªå…ˆä½¿ç”¨é€™å€‹ï¼‰
            next_button = self.driver.find_element(By.XPATH, '//*[@id="ContentPlaceHolder1_ListView1_DataPager1"]/input[2]')
            if next_button and next_button.is_enabled():
                print(f"    ðŸ–±ï¸ é»žæ“Šä¸‹ä¸€é æŒ‰éˆ• (XPath)")
                self.driver.execute_script("arguments[0].click();", next_button)

                # æ›´å¼·çš„ç­‰å¾…é‚è¼¯
                try:
                    # ç­‰å¾…èˆŠè¡¨æ ¼æ¶ˆå¤±
                    self.wait.until(EC.staleness_of(table))
                    print(f"    âœ… èˆŠè¡¨æ ¼å·²æ¶ˆå¤±")
                except TimeoutException:
                    print(f"    âš ï¸ ç­‰å¾…èˆŠè¡¨æ ¼æ¶ˆå¤±é€¾æ™‚ï¼Œä½†ç¹¼çºŒ")

                # ç­‰å¾…æ–°è¡¨æ ¼å‡ºç¾ï¼Œä¸¦ç¢ºèªæœ‰è³‡æ–™
                try:
                    self.wait.until(lambda d: self.find_data_table() is not None)
                    print(f"    âœ… æ–°è¡¨æ ¼å·²å‡ºç¾")

                    # é¡å¤–ç­‰å¾…ç¢ºä¿è³‡æ–™è¼‰å…¥å®Œæˆ
                    import time
                    time.sleep(1)

                    # æª¢æŸ¥æ–°è¡¨æ ¼æ˜¯å¦æœ‰ä¸åŒçš„è³‡æ–™ï¼ˆç°¡å–®æª¢æŸ¥ï¼‰
                    new_table = self.find_data_table()
                    if new_table:
                        new_rows = new_table.find_elements(By.TAG_NAME, 'tr')
                        print(f"    ðŸ“Š æ–°è¡¨æ ¼æœ‰ {len(new_rows)} è¡Œè³‡æ–™")
                        if len(new_rows) > 1:  # è‡³å°‘æœ‰è¡¨é ­ + ä¸€ç­†è³‡æ–™
                            print(f"    âœ… ç¿»é æˆåŠŸ")
                            return True
                        else:
                            print(f"    âš ï¸ æ–°è¡¨æ ¼ä¼¼ä¹Žæ²’æœ‰è³‡æ–™")
                            return False
                    else:
                        print(f"    âŒ æ–°è¡¨æ ¼ä¸å­˜åœ¨")
                        return False

                except TimeoutException:
                    print(f"    âŒ ç­‰å¾…æ–°è¡¨æ ¼å‡ºç¾é€¾æ™‚")
                    return False
        except Exception as e:
            print(f"    æ–¹æ³• 1 å¤±æ•—: {str(e)}")

        # å¦‚æžœæ–¹æ³• 1 å¤±æ•—ï¼Œé‡æ–°ç²å–è¡¨æ ¼å¼•ç”¨
        table = self.find_data_table()
        if not table:
            return False

        try:
            # æ–¹æ³• 2: ä½¿ç”¨ CSS é¸æ“‡å™¨
            next_button = self.driver.find_element(By.CSS_SELECTOR, "input.imgPage.nimgPage[value='>']")
            if next_button and next_button.is_enabled():
                print(f"    ðŸ–±ï¸ é»žæ“Šä¸‹ä¸€é æŒ‰éˆ• (CSS)")
                self.driver.execute_script("arguments[0].click();", next_button)
                self.wait.until(EC.staleness_of(table))
                self.wait.until(lambda d: self.find_data_table() is not None)
                print(f"    âœ… ç¿»é æˆåŠŸ")
                return True
        except Exception as e:
            print(f"    æ–¹æ³• 2 å¤±æ•—: {str(e)}")

        # å¦‚æžœæ–¹æ³• 2 å¤±æ•—ï¼Œé‡æ–°ç²å–è¡¨æ ¼å¼•ç”¨
        table = self.find_data_table()
        if not table:
            return False

        try:
            # æ–¹æ³• 3: ä½¿ç”¨ value='&gt;' ä½œç‚ºå‚™ç”¨
            next_button = self.driver.find_element(By.XPATH, "//input[@value='&gt;' and @type='submit']")
            if next_button and next_button.is_enabled():
                print(f"    ðŸ–±ï¸ é»žæ“Šä¸‹ä¸€é æŒ‰éˆ• (value)")
                self.driver.execute_script("arguments[0].click();", next_button)
                self.wait.until(EC.staleness_of(table))
                self.wait.until(lambda d: self.find_data_table() is not None)
                print(f"    âœ… ç¿»é æˆåŠŸ")
                return True
        except Exception as e:
            print(f"    æ–¹æ³• 3 å¤±æ•—: {str(e)}")

        print(f"    âŒ æ‰€æœ‰ç¿»é æ–¹æ³•éƒ½å¤±æ•—äº†")
        return False
    
    def get_current_page_number(self):
        """èŽ·å–å½“å‰é¡µç """
        try:
            page_info = self.driver.find_element(By.XPATH, "//div[contains(text(), 'é æ•¸ï¼š')]")
            text = page_info.text
            match = re.search(r'é æ•¸ï¼š\s*(\d+)/(\d+)', text)
            if match:
                return int(match.group(1)), int(match.group(2))
        except:
            pass
        return None, None
    
    def parse_table_data(self, keywords=None, follow_detail=True, extract_detail=True, current_page=1):
        """
        è§£æžå½“å‰é¡µé¢çš„è¡¨æ ¼èµ„æ–™

        :param keywords: é—œéµå­—éŽæ¿¾
        :param follow_detail: æ˜¯å¦é»žå…¥è©³æƒ…é æŠ“çœŸæ­£ç¶²å€
        :param extract_detail: æ˜¯å¦å¾žè©³æƒ…é æŠ“å–é¡å¤–è³‡è¨Šï¼ˆé ç®—ã€æ¡ˆè™Ÿç­‰ï¼‰
        """
        data = []

        table = self.find_data_table()
        if not table:
            return data

        # åˆ¤æ–·ç›®å‰æ˜¯å“ªå€‹é é¢ï¼ˆå…¬å‘Šä¸­ vs å·²ç™»è¼‰ï¼‰
        current_url = self.driver.current_url
        is_announce_page = 'inv_ann.aspx' in current_url  # å…¬å‘Šä¸­
        is_registered_page = 'inv_case.aspx' in current_url  # å·²ç™»è¼‰

        # å…ˆæŠ“ä¸€æ¬¡ç¸½åˆ—æ•¸ï¼Œè¿´åœˆç”¨ indexï¼›çœŸæ­£å– row æ™‚æœƒæ¯åœˆé‡æ–°æŠ“ï¼Œé¿å… back() ä¹‹å¾Œ element å¤±æ•ˆ
        rows = table.find_elements(By.TAG_NAME, 'tr')
        row_count = len(rows)
        print(f"  ç™¼ç¾ {row_count-1} ç­†è³‡æ–™åˆ—ï¼ˆåŒ…å«è¡¨é ­ï¼‰")

        # å¾ž 1 é–‹å§‹ï¼š0 æ˜¯è¡¨é ­
        for row_index in range(1, row_count):
            try:
                # æ¯ä¸€åœˆé‡æ–°æ‹¿ä¸€æ¬¡æœ€æ–°çš„ rowï¼Œé¿å… StaleElementReference
                table = self.find_data_table()
                if not table:
                    break

                rows = table.find_elements(By.TAG_NAME, 'tr')
                if row_index >= len(rows):
                    break

                row = rows[row_index]
                cols = row.find_elements(By.TAG_NAME, 'td')

                if len(cols) < 5:
                    continue

                # æ ¹æ“šé é¢é¡žåž‹ä½¿ç”¨ä¸åŒçš„æ¬„ä½æ˜ å°„
                if is_announce_page:
                    # å…¬å‘Šä¸­é é¢çš„æ¬„ä½é †åºï¼ˆå…±8æ¬„ï¼‰
                    case_number = cols[0].text.strip()  # æ¡ˆè™Ÿ
                    name = cols[1].text.strip()  # æ¡ˆä»¶åç¨±
                    agency = cols[2].text.strip() if len(cols) > 2 else ''  # ä¸»è¾¦æ©Ÿé—œ
                    planning_method = cols[3].text.strip() if len(cols) > 3 else ''  # è¦åŠƒæ–¹å¼
                    announcement_type = cols[4].text.strip() if len(cols) > 4 else ''  # å…¬å‘Šé¡žåˆ¥
                    announcement_count = cols[5].text.strip() if len(cols) > 5 else ''  # å…¬å‘Šæ¬¡æ•¸
                    announcement_start_date = cols[6].text.strip() if len(cols) > 6 else ''  # å…¬å‘Šé–‹å§‹æ—¥æœŸ
                    announcement_end_date = cols[7].text.strip() if len(cols) > 7 else ''  # å…¬å‘Šæˆªæ­¢æ—¥æœŸ
                    # è¨­å®šä¸»è¦æ—¥æœŸç‚ºé–‹å§‹æ—¥æœŸ
                    date = announcement_start_date

                elif is_registered_page:
                    # å·²ç™»è¼‰é é¢çš„æ¬„ä½é †åºï¼ˆå…±6æ¬„ï¼‰
                    name = cols[0].text.strip()  # æ¡ˆä»¶åç¨±
                    announcement_type = cols[1].text.strip() if len(cols) > 1 else ''  # æ¡ˆä»¶é¡žåˆ¥/ç‹€æ…‹
                    agency = cols[2].text.strip() if len(cols) > 2 else ''  # ä¸»è¾¦æ©Ÿé—œ
                    planning_method = cols[3].text.strip() if len(cols) > 3 else ''  # è¦åŠƒæ–¹å¼
                    registered_date = cols[4].text.strip() if len(cols) > 4 else ''  # å·²ç™»è¼‰æ—¥æœŸ
                    contract_date = cols[5].text.strip() if len(cols) > 5 else ''  # ç°½ç´„æ—¥æœŸ
                    case_number = ''  # å·²ç™»è¼‰é é¢é€šå¸¸æ²’æœ‰æ˜Žç¢ºçš„æ¡ˆè™Ÿ
                    # è¨­å®šä¸»è¦æ—¥æœŸç‚ºç™»è¼‰æ—¥æœŸ
                    date = registered_date

                else:
                    # é è¨­é‚è¼¯ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                    case_number = cols[0].text.strip()
                    name = cols[1].text.strip()
                    agency = cols[2].text.strip() if len(cols) > 2 else ''
                    planning_method = cols[3].text.strip() if len(cols) > 3 else ''
                    announcement_type = cols[4].text.strip() if len(cols) > 4 else ''
                    date = cols[5].text.strip() if len(cols) > 5 else ''
                    # åˆå§‹åŒ–å…¶ä»–è®Šæ•¸
                    announcement_count = ''
                    announcement_start_date = ''
                    announcement_end_date = ''
                    registered_date = ''
                    contract_date = ''

                # é©—è­‰å¿…è¦æ¬„ä½ï¼ˆè‡³å°‘è¦æœ‰æ¡ˆä»¶åç¨±ï¼‰
                if not name:
                    continue

                # å…³é”®å­—è¿‡æ»¤
                if keywords and not self.match_keywords(name + agency, keywords):
                    continue

                # é è¨­å…ˆçµ¦åˆ—è¡¨é ç¶²å€ï¼ˆè‡³å°‘ä¸æœƒæ˜¯éŒ¯çš„å‡æ·±é€£çµï¼‰
                link = self.current_list_url or self.driver.current_url
                detail_info = {}

                # å¦‚æžœè¦ç²¾æº–å€‹æ¡ˆç¶²å€ï¼ˆå’Œè©³ç´°è³‡è¨Šï¼‰ï¼Œå°±çœŸçš„é»žé€²åŽ»æ‹¿
                if follow_detail:
                    try:
                        if extract_detail:
                            # é»žé€²åŽ»æ‹¿ URL + è©³ç´°è³‡è¨Šï¼Œå‚³éžç•¶å‰é ç¢¼
                            link, detail_info = self.get_detail_url_and_info_from_row(row_index, current_page)
                        else:
                            # åªæ‹¿ URLï¼Œä¸æŠ“è©³ç´°è³‡è¨Š
                            link = self.get_detail_url_from_row(row_index)
                    except Exception as e:
                        # å¤±æ•—å°±ç¶­æŒåˆ—è¡¨é ç¶²å€
                        print(f"    âš  æŠ“å–ç¬¬ {row_index} ç­†è©³æƒ…å¤±æ•—: {str(e)}")
                        link = self.current_list_url or self.driver.current_url

                # å»ºç«‹è³‡æ–™é …ç›®ï¼Œæ¬„ä½åç¨±å°é½Š tender_announcement æ ¼å¼
                item = {
                    'serial_no': str(row_index),  # åºè™Ÿ
                    'agency': agency,  # æ©Ÿé—œåç¨±
                    'tenderId': case_number,  # æ¨™æ¡ˆç·¨è™Ÿï¼ˆæ¡ˆè™Ÿï¼‰
                    'tenderName': name,  # æ¨™æ¡ˆåç¨±
                    'transmission_count': announcement_count if is_announce_page else '',  # å‚³è¼¸æ¬¡æ•¸ï¼ˆå…¬å‘Šæ¬¡æ•¸ï¼‰
                    'tender_method': planning_method,  # æ‹›æ¨™æ–¹å¼ï¼ˆè¦åŠƒæ–¹å¼ï¼‰
                    'procurement_type': announcement_type,  # æŽ¡è³¼é¡žåˆ¥ï¼ˆå…¬å‘Šé¡žåˆ¥/æ¡ˆä»¶ç‹€æ…‹ï¼‰
                    'announcement_date': date,  # å…¬å‘Šæ—¥æœŸ
                    'deadline': announcement_end_date if is_announce_page else '',  # æˆªæ­¢æ—¥æœŸ
                    'budget_amount': '',  # é ç®—é‡‘é¡ï¼ˆç¨å¾Œå¾žè©³æƒ…é å¡«å……ï¼‰
                    'sourceUrl': link,  # ä¾†æºç¶²å€
                    'detail_url': link,  # è©³ç´°ç¶²å€
                    'detail_fetched': True,  # æ˜¯å¦å·²æŠ“å–è©³ç´°è³‡æ–™
                }

                # æ·»åŠ ä¿ƒåƒç‰¹æœ‰çš„æ¬„ä½
                item['caseNumber'] = case_number  # ä¿ç•™åŽŸæœ‰æ¬„ä½ä»¥å‘å¾Œå…¼å®¹
                item['planningMethod'] = planning_method
                item['announcementType'] = announcement_type

                # æ·»åŠ å…¬å‘Šä¸­é é¢çš„ç‰¹å®šæ¬„ä½ï¼ˆä½¿ç”¨ä¸åŒçš„æ¬„ä½åç¨±é¿å…é‡è¤‡ï¼‰
                if is_announce_page:
                    item['announcementStartDate'] = announcement_start_date
                    item['announcementEndDate'] = announcement_end_date

                # æ·»åŠ å·²ç™»è¼‰é é¢çš„ç‰¹å®šæ¬„ä½
                if is_registered_page:
                    item['registeredDate'] = registered_date
                    item['contractDate'] = contract_date

                # å¦‚æžœæœ‰å¾žè©³æƒ…é æŠ“åˆ°é¡å¤–è³‡è¨Šï¼Œåˆä½µé€²åŽ»
                if detail_info:
                    if detail_info.get('detailCaseNumber'):
                        item['detailCaseNumber'] = detail_info['detailCaseNumber']
                        # å¦‚æžœåˆ—è¡¨é æ²’æœ‰æ¡ˆè™Ÿï¼Œç”¨è©³æƒ…é çš„
                        if not item['caseNumber']:
                            item['caseNumber'] = detail_info['detailCaseNumber']
                            item['tenderId'] = detail_info['detailCaseNumber']  # åŒæ™‚æ›´æ–° tenderId
                    if detail_info.get('budget'):
                        item['budget'] = detail_info['budget']
                        item['budget_amount'] = detail_info['budget']  # å°é½Šæ¬„ä½åç¨±
                    if detail_info.get('budgetAmount') is not None:
                        item['budgetAmount'] = detail_info['budgetAmount']

                data.append(item)

            except StaleElementReferenceException:
                # DOM è®Šå‹•å°Žè‡´å…ƒç´ å¤±æ•ˆï¼Œé€™ç­†å°±å…ˆç•¥éŽ
                print(f"    âš  ç¬¬ {row_index} ç­†è³‡æ–™å¤±æ•ˆï¼Œç•¥éŽ")
                continue
            except Exception as e:
                print(f"    âš  è™•ç†ç¬¬ {row_index} ç­†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue

        return data
    
    def scrape_with_autopagination(self, url, status_label, keywords=None, 
                                   follow_detail=True, extract_detail=True, max_pages=20):
        """
        è‡ªåŠ¨ç¿»é¡µæŠ“å–æ‰€æœ‰èµ„æ–™
        
        :param url: åˆ—è¡¨é ç¶²å€
        :param status_label: ç‹€æ…‹æ¨™ç±¤ï¼ˆå…¬å‘Šä¸­/å·²ç™»è¼‰ï¼‰
        :param keywords: é—œéµå­—éŽæ¿¾
        :param follow_detail: æ˜¯å¦é»žå…¥è©³æƒ…é æŠ“çœŸæ­£ç¶²å€
        :param extract_detail: æ˜¯å¦å¾žè©³æƒ…é æŠ“å–é¡å¤–è³‡è¨Š
        :param max_pages: æœ€å¤§ç¿»é æ•¸
        """
        print("\n" + "="*70)
        print(f"ðŸ“¢ å¼€å§‹æŠ“å–ä¿ƒå‚{status_label}æ¡ˆä»¶ï¼ˆè¯¦ç»†ç‰ˆï¼‰...")
        if follow_detail:
            print(f"   âš™ï¸  æ¨¡å¼ï¼šé»žå…¥è©³æƒ…é æŠ“å–å®Œæ•´è³‡è¨Šï¼ˆæ‰‹å‹•åˆ‡æ›é é¢ï¼‰")
        else:
            print(f"   âš™ï¸  æ¨¡å¼ï¼šåƒ…æŠ“å–åˆ—è¡¨é è³‡è¨Šï¼ˆå¿«é€Ÿï¼‰")
        print("="*70)

        all_data = []

        try:
            # è¨˜éŒ„é€™æ¬¡çš„åˆ—è¡¨ç¶²å€
            self.current_list_url = url

            self.driver.get(url)
            print(f"âœ“ æˆåŠŸè®¿é—®: {url}")
            print("  ç­‰å¾…é¡µé¢åŠ è½½...")
            self.wait.until(lambda d: self.find_data_table() is not None)
            
            page_num = 1
            
            while page_num <= max_pages:
                print(f"\næ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ...")

                # è§£æžå½“å‰é¡µé¢
                page_data = self.parse_table_data(
                    keywords=keywords,
                    follow_detail=follow_detail,
                    extract_detail=extract_detail,
                    current_page=page_num
                )

                if page_data:
                    # ä¸ºæ¯ç¬”èµ„æ–™åŠ ä¸ŠçŠ¶æ€å’Œç±»åž‹
                    for item in page_data:
                        item['status'] = status_label
                        item['type'] = 'ä¿ƒå‚æ¡ˆä»¶'

                    all_data.extend(page_data)
                    print(f"  âœ“ æ”¶é›† {len(page_data)} ç¬”èµ„æ–™ï¼ˆç´¯è®¡ {len(all_data)} ç¬”ï¼‰")
                else:
                    print(f"  âš  æœ¬é¡µæœªæ‰¾åˆ°èµ„æ–™")

                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                if self.has_next_page():
                    print(f"  â†’ å‡†å¤‡ç¿»åˆ°ä¸‹ä¸€é¡µ...")
                    if self.click_next_page():
                        page_num += 1
                        # æ›´æ–°ç•¶å‰é é¢ URL
                        self.current_list_url = self.driver.current_url
                        print(f"  âœ… æˆåŠŸç¿»åˆ°ç¬¬ {page_num} é¡µï¼ŒURL: {self.current_list_url}")
                    else:
                        print("  âš  æ— æ³•ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œç»“æŸç¿»é¡µ")
                        break
                else:
                    print(f"  âœ“ å·²åˆ°è¾¾æœ€åŽä¸€é¡µ")
                    break
            
            if page_num > max_pages:
                print(f"\nâš  è¾¾åˆ°å®‰å…¨ä¸Šé™ï¼ˆ{max_pages} é¡µï¼‰ï¼Œåœæ­¢ç¿»é¡µ")
            
            print(f"\nâœ“ æˆåŠŸæ”¶é›† {len(all_data)} ç¬”ä¿ƒå‚{status_label}æ¡ˆä»¶")
            return all_data
            
        except Exception as e:
            print(f"âœ— æŠ“å–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return all_data
    
    def collect_all_categories(self, keywords=None, follow_detail=True, 
                              extract_detail=True, max_pages=20):
        """
        ä¾åºæŠ“å–ä¿ƒåƒå…¬å‘Šèˆ‡å·²ç™»è¼‰æ¡ˆä»¶ï¼Œå›žå‚³çµ±ä¸€è³‡æ–™çµæ§‹ã€‚

        :param keywords: é—œéµå­—åˆ—è¡¨ï¼Œç”¨æ–¼åç¨±/æ©Ÿé—œéŽæ¿¾
        :param follow_detail: æ˜¯å¦é»žå…¥è©³æƒ…é æŠ“çœŸæ­£ç¶²å€
        :param extract_detail: æ˜¯å¦å¾žè©³æƒ…é æŠ“å–é¡å¤–è³‡è¨Š
        :param max_pages: æ¯å€‹é¡žåˆ¥æœ€å¤§ç¿»é æ•¸
        :return: dictï¼ŒåŒ…å« promotionAnnounceã€promotionRegistered
        """
        all_data = {
            'promotionAnnounce': [],
            'promotionRegistered': []
        }

        try:
            all_data['promotionAnnounce'] = self.scrape_with_autopagination(
                'https://ppp.mof.gov.tw/WWW/inv_ann.aspx',
                'å…¬å‘Šä¸­',
                keywords=keywords,
                follow_detail=follow_detail,
                extract_detail=extract_detail,
                max_pages=max_pages
            )
            
            all_data['promotionRegistered'] = self.scrape_with_autopagination(
                'https://ppp.mof.gov.tw/WWW/inv_case.aspx',
                'å·²ç™»è½½',
                keywords=keywords,
                follow_detail=follow_detail,
                extract_detail=extract_detail,
                max_pages=max_pages
            )
        except Exception as exc:
            print(f"âœ— æŠ“å–éŽç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{exc}")
            raise

        return all_data

    def match_keywords(self, text, keywords):
        """å…³é”®å­—åŒ¹é…"""
        if not keywords:
            return True
        text_lower = text.lower()
        return any(kw.lower() in text_lower for kw in keywords)
    
    def save_to_json(self, data, filename):
        """å‚¨å­˜ä¸º JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ JSON å·²å‚¨å­˜: {filename}")
    
    def save_to_csv(self, all_data, base_filename):
        """å‚¨å­˜ä¸º CSV"""
        for key, data in all_data.items():
            if data:
                filename = f"{base_filename}_{key}.csv"
                try:
                    # å‹•æ…‹å–å¾—æ‰€æœ‰å¯èƒ½çš„æ¬„ä½
                    fieldnames = set()
                    for item in data:
                        fieldnames.update(item.keys())
                    fieldnames = sorted(list(fieldnames))
                    
                    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        for item in data:
                            writer.writerow(item)
                    print(f"âœ“ CSV å·²å‚¨å­˜: {filename}")
                except Exception as e:
                    print(f"âœ— CSV å‚¨å­˜å¤±è´¥: {str(e)}")


def _build_result_payload(
    data: Dict[str, List[Dict[str, Any]]],
    *,
    keywords: Optional[List[str]] = None
) -> Dict[str, Any]:
    """æ•´ç†çµ±ä¸€è¼¸å‡ºæ ¼å¼ï¼ŒåŒ…å«çµ±è¨ˆè³‡è¨Šã€‚"""
    timestamp = datetime.now()
    stats = {
        key: len(value)
        for key, value in data.items()
    }
    total = sum(stats.values())
    return {
        "crawlerId": "promotion-platform-detailed",
        "runAt": timestamp.isoformat(),
        "filters": {
            "keywords": keywords or [],
        },
        "stats": stats,
        "totalRecords": total,
        "data": data,
    }


def run_promotions(
    headless: bool = True,
    keywords: Optional[List[str]] = None,
    output_dir: Optional[Path] = None,
    follow_detail: bool = True,
    extract_detail: bool = True,
    max_pages: int = 20
) -> Dict[str, Any]:
    """
    ä¾›å¤–éƒ¨å‘¼å«çš„å¯¦ç”¨å‡½å¼ï¼š
    - headless: æ˜¯å¦å•Ÿç”¨ Headless æ¨¡å¼ï¼ˆé è¨­ Trueï¼Œæ–¹ä¾¿è‡ªå‹•åŒ–ï¼‰
    - keywords: é—œéµå­—éŽæ¿¾ï¼›None ä»£è¡¨æ“·å–å…¨éƒ¨
    - output_dir: è‹¥æŒ‡å®šå‰‡åœ¨è©²è³‡æ–™å¤¾åº•ä¸‹è¼¸å‡º JSON æª”æ¡ˆ
    - follow_detail: æ˜¯å¦é»žå…¥è©³æƒ…é æŠ“çœŸæ­£ç¶²å€
    - extract_detail: æ˜¯å¦å¾žè©³æƒ…é æŠ“å–é¡å¤–è³‡è¨Š
    - max_pages: æ¯å€‹é¡žåˆ¥æœ€å¤§ç¿»é æ•¸

    :return: åŒ…å«è³‡æ–™èˆ‡çµ±è¨ˆè³‡è¨Šçš„ dict
    """
    scraper = ProcurementScraperDetailed(headless=headless)
    try:
        scraper.setup_driver()
        data = scraper.collect_all_categories(
            keywords=keywords,
            follow_detail=follow_detail,
            extract_detail=extract_detail,
            max_pages=max_pages
        )
        result = _build_result_payload(data, keywords=keywords)

        if output_dir:
            output_dir.mkdir(parents=True, exist_ok=True)
            filename = output_dir / f"promotion_platform_detailed_{datetime.now():%Y%m%d_%H%M%S}.json"
            scraper.save_to_json(result, str(filename))

        return result
    finally:
        scraper.close_driver()


def main():
    print("\n" + "="*70)
    print("ðŸ¢ ECOVE æ”¿åºœæ ‡æ¡ˆèµ„è®¯æ”¶é›†ç³»ç»Ÿ - è©³ç´°ç‰ˆ")
    print("   æœƒé»žé€²æ¯ç­†æ¡ˆä»¶çš„è©³æƒ…é ï¼Œå–å¾—çœŸæ­£çš„ç¶²å€å’Œå®Œæ•´è³‡è¨Š")
    print("="*70 + "\n")
    
    # è®¾å®šå‚æ•°
    print("âš™ï¸  è¨­å®šåƒæ•¸ï¼š")
    print("="*70)
    
    # å…³é”®å­—è®¾å®šï¼ˆNone = æ”¶é›†æ‰€æœ‰æ¡ˆä»¶ï¼‰
    keywords = None
    print(f"  é—œéµå­—éŽæ¿¾: {'ç„¡ï¼ˆæŠ“å–å…¨éƒ¨ï¼‰' if not keywords else ', '.join(keywords)}")
    
    # æ˜¯å¦é»žé€²è©³æƒ…é ï¼ˆé è¨­ç‚º Trueï¼Œä½¿ç”¨æ‰‹å‹•åˆ‡æ›é é¢æ–¹å¼ï¼‰
    follow_detail = True
    print(f"  é»žå…¥è©³æƒ…é : {'æ˜¯ï¼ˆæ‰‹å‹•åˆ‡æ›é é¢ï¼‰' if follow_detail else 'å¦'}")

    # æ˜¯å¦æŠ“å–è©³ç´°è³‡è¨Šï¼ˆé ç®—ã€æ¡ˆè™Ÿç­‰ï¼‰
    extract_detail = True
    print(f"  æŠ“å–è©³ç´°è³‡è¨Š: {'æ˜¯ï¼ˆé ç®—ã€æ¡ˆè™Ÿç­‰ï¼‰' if extract_detail else 'å¦'}")
    
    # æœ€å¤§ç¿»é æ•¸
    max_pages = 5  # æ¸¬è©¦æ™‚å¯ä»¥è¨­å°ä¸€é»žï¼Œæ­£å¼ä½¿ç”¨æ”¹æˆ 20 æˆ–æ›´å¤§
    print(f"  æœ€å¤§ç¿»é æ•¸: {max_pages} é /é¡žåˆ¥")
    
    print("="*70 + "\n")
    
    if follow_detail:
        print("âš ï¸  æ³¨æ„ï¼šé»žå…¥è©³æƒ…é æœƒæ¯”è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å€™...")
        print("   å¦‚éœ€å¿«é€Ÿæ¸¬è©¦ï¼Œå¯å°‡ follow_detail è¨­ç‚º False\n")
    
    scraper = ProcurementScraperDetailed(headless=False)
    all_data = {
        'promotionAnnounce': [],
        'promotionRegistered': []
    }
    run_result = {}

    try:
        scraper.setup_driver()
        
        all_data = scraper.collect_all_categories(
            keywords=keywords,
            follow_detail=follow_detail,
            extract_detail=extract_detail,
            max_pages=max_pages
        )
        run_result = _build_result_payload(all_data, keywords=keywords)
        
        # å‚¨å­˜ç»“æžœ
        print("\n" + "="*70)
        print("ðŸ’¾ å‚¨å­˜æ”¶é›†ç»“æžœ...")
        print("="*70)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        scraper.save_to_json(run_result, f'procurement_data_detailed_{timestamp}.json')
        
        # ç»Ÿè®¡
        print("\n" + "="*70)
        print("ðŸ“Š æ”¶é›†å®Œæˆï¼ç»Ÿè®¡èµ„è®¯ï¼š")
        print("="*70)
        
        total = run_result.get("totalRecords", 0)
        for key, value in all_data.items():
            count = len(value)
            if count > 0:
                type_name = {
                    'promotionAnnounce': 'ä¿ƒå‚å…¬å‘Š',
                    'promotionRegistered': 'ä¿ƒå‚ç™»è½½'
                }.get(key, key)
                print(f"  {type_name}: {count} ç¬”")
                
                # é¡¯ç¤ºæ˜¯å¦æœ‰æŠ“åˆ°è©³ç´°è³‡è¨Š
                if extract_detail and value:
                    with_budget = sum(1 for item in value if item.get('budget'))
                    with_detail_case_no = sum(1 for item in value if item.get('detailCaseNumber'))
                    print(f"    â””â”€ å«é ç®—è³‡è¨Š: {with_budget} ç­†")
                    print(f"    â””â”€ å«è©³ç´°æ¡ˆè™Ÿ: {with_detail_case_no} ç­†")
        
        print(f"\n  ðŸ“Œ æ€»è®¡: {total} ç¬”èµ„æ–™")
        print(f"  âœ“ èµ„æ–™å·²å‚¨å­˜ä¸º JSON æ ¼å¼")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\nâœ— æ‰§è¡Œé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        scraper.close_driver()
    
    # æ‰¹æ¬¡åŸ·è¡Œæ™‚ä¸æš«åœï¼Œè®“ç¨‹å¼è‡ªå‹•çµæŸ


if __name__ == '__main__':
    main()





